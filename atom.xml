<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-06-19T10:33:33.343Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>yu-qinping</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>在kafka集群中部署Eagle运维监控</title>
    <link href="http://example.com/2022/06/13/kafka_eagle/"/>
    <id>http://example.com/2022/06/13/kafka_eagle/</id>
    <published>2022-06-13T02:00:41.000Z</published>
    <updated>2022-06-19T10:33:33.343Z</updated>
    
    <content type="html"><![CDATA[<h1 id="在kafka集群中部署Eagle运维监控"><a href="#在kafka集群中部署Eagle运维监控" class="headerlink" title="在kafka集群中部署Eagle运维监控"></a>在kafka集群中部署Eagle运维监控</h1><hr><p>title: 在kafka集群中部署Eagle运维监控</p><details><summary>阅读全文</summary><p>1、根据学习通资料下载</p><p>2、解压安装包</p><blockquote><blockquote><blockquote></blockquote><pre><code>[root@node1 ~]#unzip kafka-eagle-bin-2.1.0.tar.gz[root@node1 ~]#unzip kafka-eagle-web-2.0.2-bin.tar.gz</code></pre></blockquote></blockquote><p>3、进入kafka-eagle-web-2.0.2</p><blockquote><blockquote><blockquote></blockquote><pre><code>[root@node1 ~]#cd kafka-eagle-web-2.0.2</code></pre></blockquote></blockquote><p>4、找到目录</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 kafka-eagle-web-2.0.2 ]#pwd<br>   /export/server/kafka-eagle/kafka-eagle-web</p></blockquote></blockquote><p>5、配置环境变量</p><blockquote><blockquote><blockquote></blockquote><p>   vi /etc/profile<br>   export JAVA_HOME=/usr/java/jdk1.8<br>   export PATH=$PATH:$JAVA_HOME/bin<br>   export KE_HOME= /export/server/kafka-eagle/kafka-eagle-web<br>   export PATH=$PATH:$KE_HOME/bin</p></blockquote></blockquote><p>6、配置生效</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 kafka-eagle-web-2.0.2 ]#. /etc/profile</p></blockquote></blockquote><p>7.通过status查看</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 kafka-eagle-web-2.0.2 ]#ke.sh status<br>   [2022-06-05 16:55:05] INFO :kafka Eagle Has Stopped,[563715] .</p></blockquote></blockquote><p>8.配置 KafkaEagle</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 kafka-eagle-web-2.0.2 ]#cd /export/server/kafka-eagle/kafka-eagle-web/conf<br>   [root@node1 conf]#vi system-config.properties</p></blockquote></blockquote><p>需要更改的地方：<br>    &gt;&gt;&gt;<br>   kafka.eagle.zk.cluster.alias=cluster1<br>   cluster1.zk.list=node1:2181,node2:2181,node3:2181<br>   cluster1.kafka.eagle.broker.size=3</p><p>   kafka.eagle.url=jdbc:sqlite:/export/data/db/ke.db</p><p>9启动前需要手动创建/export/data/db目录</p><blockquote><blockquote><blockquote><p>[root@node1 ~]mkdir /export/data/db</p></blockquote></blockquote></blockquote><p>10.启动zookeeper</p><blockquote><blockquote><blockquote><p>[root@node1 ~]zkServer.sh start</p></blockquote></blockquote></blockquote><p>11启动kafka</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 ~]cd /export/server/kafka/<br>   [root@node1 ~]bin/kafka-server-start.sh -daemon config/server.properties</p></blockquote></blockquote><p>12启动Eagle</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 ~]/export/server/kafka-eagle/bin/ke.sh start</p></blockquote></blockquote><p><img src="../images/31.png" alt="31"><br><img src="../images/35.jpg" alt="35"><br><img src="../images/32.jpg" alt="32"></p><p>14、eagle使用功能介绍</p><p>（1）我们还可以直接通过kafka-eagle来发送消息；<br><img src="../images/36.jpg" alt="36"></p><p>（2）KSQL功能，可以通过SQL语句来查询Topic中的消息；<br><img src="../images/39.jpg" alt="39"></p><p>（3）可视化工具自然少不了监控，如果你想开启kafka-eagle对Kafka的监控功能的话，需要修改Kafka的启动脚本，暴露JMX的端口；</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 kafka-eagle-web-2.0.2 ]#vi kafka-server-start.sh</p></blockquote></blockquote><h1 id="暴露JMX端口"><a href="#暴露JMX端口" class="headerlink" title="暴露JMX端口"></a>暴露JMX端口</h1><p>   if [ “x$KAFKA_HEAP_OPTS” = “x” ]; then<br>       export KAFKA_HEAP_OPTS=”-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70”<br>       export JMX_PORT=”9999”<br>   fi </p><p>（4）监控图表界面；如下：<br><img src="../images/37.jpg" alt="37"></p><p>（5）监控大屏功能；<br><img src="../images/38.jpg" alt="38"></p><p>**</p></details><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;在kafka集群中部署Eagle运维监控&quot;&gt;&lt;a href=&quot;#在kafka集群中部署Eagle运维监控&quot; class=&quot;headerlink&quot; title=&quot;在kafka集群中部署Eagle运维监控&quot;&gt;&lt;/a&gt;在kafka集群中部署Eagle运维监控&lt;/h1&gt;&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>kafka API使用方法</title>
    <link href="http://example.com/2022/06/07/Kafka%20API%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>http://example.com/2022/06/07/Kafka%20API%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</id>
    <published>2022-06-07T10:20:00.000Z</published>
    <updated>2022-06-19T10:26:38.733Z</updated>
    
    <content type="html"><![CDATA[<h1 id="kafka-API使用方法"><a href="#kafka-API使用方法" class="headerlink" title="kafka API使用方法"></a>kafka API使用方法</h1><hr><p>title: kafka API使用方法</p><details><summary>阅读全文</summary><p>1、一个正常的生产逻辑需要具备以下几个步骤</p><blockquote><blockquote><blockquote></blockquote><pre><code>（1）、配置生产者客户端参数及创建相应的生产者实例；（2）、构建待发送的消息；（3）、发送消息；（4）、关闭生产者实例。</code></pre></blockquote></blockquote><p>2、生产者API采用默认分区方式将消息散列的发送到各个分区中。</p><p>3、什么时候会发生重复写入：producer的重试机制中，检测到一条数据的发送失败，而实际上已经发送成功，只是因为服务端响应超时。</p><p>4、什么时候会发生数据写入的丢失：ack参数的配置。</p><p>5、ack模式：取值0，1，all</p><blockquote><blockquote><blockquote></blockquote><pre><code>0代表producer往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是销量最高。1代表producer往集群发送数据只要leader成功写入消息就可以发送下一条，只确保leader接收成功。-1或all代表producer往集群发送数据需要所有的ISR Follow都完成从Leader的同步才会发送下一条，确保leader发送成功和所有的副本都成功接收。安全性最高，但是效率最低。</code></pre></blockquote></blockquote><p>6、Properties props = new Properties(); -&gt;配置生产者客户端参数</p><p>7、props.put(“bootstrap.servers”, “node1:9092,node2:9092,node3:9092”);-&gt;设置 kafka 集群的地址</p><p>8、props.put(“retries”, 3); -&gt;失败重试次数</p><p>9、props.put(“batch.size”, 10); -&gt;数据发送的批次大小</p><p>10、props.put(“linger.ms”, 10000); -&gt;消息在缓冲区保留的时间，超过设置的值就会被提交到服务端。</p><p>11、props.put(“max.request.size”,10); -&gt;数据发送请求的最大缓存数</p><p>12、props.put(“buffer.memory”, 10240); -&gt;整个 Producer 用到总内存的大小，如果缓冲区满了会提交数据到服务端//buffer.memory 要大于 batch.size，否则会报申请内存不足的错误，降低阻塞的可能性。</p><p>13、props.put(“key.serializer”, “org.apache.kafka.common.serialization.StringSerializer”); -&gt;key-value序列化器</p><p>14、props.put(“value.serializer”, “org.apache.kafka.common.serialization.StringSerializer”); -&gt;字符串最好</p><p>15、在 Kafka 生产者客户端 KatkaProducer 中有3个参数是必填的 -&gt; bootstrap.servers、key.serializer、value.serializer</p><p>16、生产者api参数发送方式（发后即忘）：</p><blockquote><blockquote><blockquote></blockquote><p>   发后即忘，它只管往 Kafka 发送,并不关心消息是否正确到达。在大多数情况下，这种发送方式没有问题; 不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高,可靠性最差。<br>   ack-&gt;作用在broker<br>   Future<RecordMetadata> send = producer.send(rcd);-&gt;也是异步</p></blockquote></blockquote><p>17、生产者api参数发送方式（同步发送）：</p><blockquote><blockquote><blockquote></blockquote><pre><code> producer.send(rcd).get(); -&gt;一旦调用get方法，就会阻塞 Future future = Callable.run()-&gt;有返回值，future.get() Runnable.run()-&gt;无返回值</code></pre></blockquote></blockquote><p>18、生产者api参数发送方式（异步发送）：</p><blockquote><blockquote><blockquote></blockquote><p>   回调函数会在 producer 收到ack时调用，为异步调用，该方法有两个参数，分别是 RecordMetadata 和Exception，如果 Exception 为 null，说明消息发送成功，如果Exception不为 null，说明消息发送失败。<br>   注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</p></blockquote></blockquote><p>19、幂等性：生产者将一条数据多次重复写入的情况下，broker端依然只有一条。</p><p>20、在IJ中新建Maven项目，配置pom.xml，重新加载Maven项目<br><img src="../images/1.png"><br><img src="../images/2.png"></p><p>21、新建ProducerDemo类、ProducerCallbackDemo类<br><img src="../images/3.png"><br><img src="../images/4.png"><br><img src="../images/5.png"><br><img src="../images/6.png"></p><p>22、生产者原理<br><img src="../images/7.png"></p><blockquote></blockquote><p>   （1）、一个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程 。<br>    （2）、在主线程中由 kafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器(RecordAccumulator，也称为消息收集器)中。<br>    （3）、Sender 线程负责从 RecordAccumulator 获取消息并将其发送到 Kafka 中；<br>    （4）、RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。<br>    （5）、RecordAccumulator 缓存的大小可以通过生产者客户端参数 buffer.memory 配置，默认值为 33554432B，即 32M。如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候 KafkaProducer.send()方法调用要么被阻塞，要么抛出异常，这个取决于参数max.block.ms 的配置，此参数的默认值为 60000，即 60 秒。<br>    （6）、主线程中发送过来的消息都会被迫加到 RecordAccumulator 的某个双端队列( Deque )中， RecordAccumulator 内部为每个分区都维护了一个双端队列，即 Deque<ProducerBatch>。消息写入缓存时,追加到双端队列的尾部。<br>    （7）、Sender 读取消息时，从双端队列的头部读取。<br>    （8）、注意：ProducerBatch 是指一个消息批次；与此同时，会将较小的 ProducerBatch 凑成一个较大 ProducerBatch，也可以减少网络请求的次数以提升整体的吞吐量。<br>    （9）、当topic中的分区数增多的情况下，recordaccumulator中的分区数就会增多。当topic的数量增多的情况下，recordaccumulator中的分区数也会增多。<br>    （10）、ProducerBatch 大小和 batch.size 参数也有着密切的关系。<br>    （11）、当一条消息(ProducerRecord) 流入RecordAccumulator时，会先寻找与消息分区所对应的双端队列(如果没有则新建)，再从这个双端队列的尾部获取一个 ProducerBatch (如果没有则新建)，查看 ProducerBatch 中是否还可以写入这个 ProducerRecord，如果可以写入，如果不可以则需要创建一个新的 Producer Batch。<br>    （12）、在新建ProducerBatch 时评估这条消息的大小是否超过 batch.size 参数大小，如果不超过，那么就以 batch.size 参数的大小来创建 ProducerBatch。如果生产者客户端需要向很多分区发送消息，则可以将 buffer.memory 参数适当调大以增加整体的吞吐量。<br>    （13）、Sender 从 RecordAccumulator 获取缓存的消息之后，会进一步将&lt;分区，Deque<Producer Batch>&gt;的形式转变成&lt;Node,List&lt; ProducerBatch&gt;的形式，其中 Node 表示 Kafka 集群 broker 节点。<br>    （14）、对于网络连接来说，生产者客户端是与具体 broker 节点建立的连接，也就是向具体的 broker 节点发送消息，而并不关心消息属于哪一个分区。<br>    （15）、而对于 KafkaProducer 的应用逻辑而言，我们只关注向哪个分区中发送哪些消息，所以在这里需要做一个应用逻辑层面到网络 I/O 层面的转换。<br>    （16）、在转换成&lt;Node, List<ProducerBatch>&gt;的形式之后，Sender 会进一步封装成&lt;Node,Request&gt; 的形式，这样就可以将 Request 请求发往各个 Node了，这里的 Request 是 Kafka 各种协议请求。<br>    （17）、请求在从sender线程发往 Kafka 之前还会保存到InFlightRequests中，InFlightRequests 保存对象的具体形式为 Map&lt;Nodeld, Deque<request>&gt;，它的主要作用是缓存了已经发出去但还没有收到服务端响应的请求(Nodeld 是一个 String 类型,表示节点的 id 编号)。<br>    （18）、与此同时，InFlightRequests 还提供了许多管理类的方法，并且通过配置参数还可以限制每个连接(也就是客户端与 Node 之间的连接) 最多缓存的请求数。<br>    （19）、这个配置参数为max.in.flight.request.per.Connection，默认值为5，即每个连接最多只能缓存5个未响应的请求，超过该数值之后就不能再向这个连接发送更多的请求了，除非有缓存的请求收到了响应( Response )。<br>    （20）、通过比较 Deque<Request> 的 size 与这个参数的大小来判断对应的 Node 中是否己经堆积了很多未响应的消息，如果真是如此，那么说明这个 Node 节点负载较大或网络连接有问题，再继其发送请求会增大请求超时的可能。</p><p>23、重要的生产者参数</p><blockquote></blockquote><p>   （1）、max.request.size<br>这个参数用来限制生产者客户端能发送的消息的最大值，默认值为1048576B，即 1MB 一般情况下，这个默认值就可以满足大多数的应用场景了。<br>这个参数还涉及一些其它参数的联动，比如 broker 端的 message.max.bytes 参数，如果配置错误可能会引起一些不必要的异常；比如将broker 端的message.max.bytes 参数配置为 10，而max.request.size 参数配置为20，那么当发送一条大小为15B的消息时，生产者客户端就会报出异常。<br>   （2）、compression.type<br>这个参数用来指定消息的压缩方式，默认值为“none”，即默认情况下，消息不会被压缩。该参数还可以配置为”gzip”，”snappy” 和 “lz4”。（服务端也有压缩参数，先解压，再压缩）；对消息进行压缩可以极大地减少网络传输、降低网络 I/O，从而提高整体的性能。消息压缩是一种以时间换空间的优化方式，如果对时延有一定的要求，则不推荐对消息进行压缩；没有必要，不需要压缩。<br>   （3）、retries 和 retry.backoff.ms<br>retries 参数用来配置生产者重试的次数，默认值为0，即在发生异常的时候不进行任何重试动作。消息在从生产者发出到成功写入服务器之前可能发生一些临时性的异常，比如网络抖动、 leader 副本的选举等，这种异常往往是可以自行恢复的，生产者可以通过配置 retries 大于 0 的值，以此通过内部重试来恢复而不是一味地将异常抛给生产者的应用程序。如果重试达到设定的次数，那么生产者就会放弃重试并返回异常。重试还和另一个参数retry.backoff.ms有关，这个参数的默认值为100，它用来设定两次重试之间的时间间隔，避免无效的频繁重试。<br>Kafka 可以保证同一个分区中的消息是有序的。如果生产者按照一定的顺序发送消息，那么这些消息也会顺序地写入分区，进而消费者也可以按照同样的顺序消费它们。对于某些应用来说，顺序性非常重要，比如 MySQL binlog 的传输，如果出现错误就会造成非常严重的后果；MySQL binlog–&gt;mysql插入数据–&gt;操作结果体会在表中–&gt;mysql为了提高可靠性会把操作记录在日志中–&gt;为了以后的主从同步（mysql集群，主表，子表）–&gt;读写分离–&gt;binlog（mysql自己设计的格式，二进制形式）。<br>如果将 acks 参数配置为非零值，并且max.flight.requests.per.connection 参数配置为大于1的值，那可能会出现错序的现象：如果第一批次消息写入失败，而第二批次消息写入成功，那么生产者会重试发送第一批次的消息，此时如果第一次的消息写入成功，那么这两个批次的消息就出现了错序。<br>一般而言，在需要保证消息顺序的场合建议把参数max.in.flight.requests.per.connection 配置为 1 ，而不是把 acks 配置为0，不过这样也会影响整体的吞吐。–&gt;吞吐量降低<br>   （4）、batch.size<br>每个Batch 要存放 batch.size 大小的数据后，才可以发送出去。比如说 batch.size 默认值是 16KB，那么里面凑够 16KB 的数据才会发送。<br>理论上来说，提升 batch.size 的大小，可以允许更多的数据缓冲在里面，那么一次 Request 发送出去的数据量就更多了，这样吞吐量可能会有所提升。但是 batch.size 也不能过大，要是数据老是缓冲在 Batch 里迟迟不发送出去，那么发送消息的延迟就会很高。一般可以尝试把这个参数调节大些，利用生产环境发消息负载测试一下。<br>   （5）、linger.ms（和batchsize有联系）<br>这个参数用来指定生产者发送 ProducerBatch 之前等待更多消息( ProducerRecord )加入ProducerBatch 时间，默认值为0。生产者客户端会在ProducerBatch 填满或等待时间超过 linger.ms 值时发送出去。增大这个参数的值会增加消息的延迟，但是同时能提升一定的吞吐量。<br>   （6）、enable.idempotence<br>是否开启幂等性功能,详见后续原理加强；幂等性,就是一个操作重复做，每次的结果都一样，x<em>1=1，x</em>1=1，x*1=1；在 kafka 中，就是生产者生产的一条消息，如果多次重复发送，在服务器中的结果还是只有一条。kafka很难实现幂等性，如果重复发，kafka肯定有多条消息–&gt;需要有机制判断曾经是否发送过–&gt;各种手段判断–&gt;事务管理的概念–&gt;加入幂等性，吞吐量会急剧下降。<br>   （7）、partitioner.classes<br>用来指定分区器，默认：org.apache.kafka.internals.DefaultPartitioner –&gt;用hashcode分<br>自定义 partitioner 需要实现 org.apache.kafka.clients.producer.Partitioner 接口 –&gt;可以通过partitioner接口自己实现分区器</p><p>四、消费者API<br>1、一个正常的消费逻辑需要具备以下几个步骤： </p><blockquote><blockquote><blockquote></blockquote><pre><code>（1）、配置消费者客户端参数（2）、创建相应的消费者实例（3）、订阅主题（4）、拉取消息并消费（5）、提交消费位移 offset（6）、关闭消费者实例</code></pre></blockquote></blockquote><p>2、subscribe重载方法：</p><blockquote><blockquote><blockquote></blockquote><pre><code>（1）、前面两种通过集合的方式订阅一到多个topic</code></pre><p>public void subscribe(Collection<String> topics,ConsumerRebalanceListener listener)<br>public void subscribe(Collection<String> topics)<br>    （2）、后两种主要是采用正则的方式订阅一到多个topic<br>public void subscribe(Pattern pattern, ConsumerRebalanceListener listener)<br>public void subscribe(Pattern pattern)<br>    （3）、正则方式订阅主题（只要是tpc_数字的形式，三位数字以内）<br>如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中，如果有人又创建了新的主题，并且主题名字与正表达式相匹配，那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题，并且可以处理不同的类型，那么这种订阅方式就很有效。利用正则表达式订阅主题，可实现动态订阅；</p></blockquote></blockquote><p>3、assign订阅主题</p><blockquote><blockquote><blockquote></blockquote><pre><code>消费者不仅可以通过 KafkaConsumer.subscribe()方法订阅主题，还可直接订阅某些主题的指定分区； 在KafkaConsumer 中提供了assign()方法来实现这些功能，此方法的具体定义如下：public void assign(Collection&lt;TopicPartition&gt; partitions) ；这个方法只接受参数 partitions，用来指定需要订阅的分区集合。示例如下: consumer.assign(Arrays.asList(new TopicPartition (&quot;tpc_1&quot;,0),new TopicPartition(“tpc_2”,1)))； </code></pre></blockquote></blockquote><p>4、subscribe与assign的区别</p><blockquote><blockquote><blockquote></blockquote><pre><code>（1）、通过 subscribe()方法订阅主题具有消费者自动再均衡功能；在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组的消费者增加或减少时，分区分配关系会自动调整，以实现消费负载均衡及故障自动转移。（2）、assign() 方法订阅分区时，是不具备消费者自动均衡的功能的； 其实这一点从 assign()方法参数可以看出端倪,两种类型subscribe()都有ConsumerRebalanceListener类型参数的方法，而assign()方法却没有。</code></pre></blockquote></blockquote><p>5、取消订阅</p><blockquote><blockquote><blockquote></blockquote><pre><code>（1）、可以使用KafkaConsumer中的unsubscribe()方法采取消主题的订阅，这个方法既可以取消通过subscribe( Collection)方式实现的订阅; （2）、也可以取消通过subscribe(Pattem)方式实现的订阅，还可以取消通过 assign(Collection)方式实现的订阅。（3）、如果将subscribe(Collection)或 assign(Collection)集合参数设置为空集合，作用与 unsubscribe()方法相同，如下示例中三行代码的效果相同：consumer.unsubscribe(); consumer.subscribe(new ArrayList&lt;String&gt;()) ; consumer.assign(new ArrayList&lt;TopicPartition&gt;());</code></pre><p>6、消息的消费模式</p><blockquote></blockquote><pre><code>Kafka中的消费是基于拉取模式的。消息的消费一般有两种模式：推送模式和拉取模式。推模式是服务端主动将消息推送给消费者，而拉模式是消费者主动向服务端发起请求来拉取消息。Kafka中的消息消费是一个不断轮询的过程，消费者所要做的就是重复地调用 poll()方法，poll()方法返回的是所订阅的主题(分区)上的一组消息。对于poll () 方法而言，如果某些分区中没有可供消费的消息，那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息，那么 poll()方法返回为空的消息集; poll ()方法具体定义如下: public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout) 超时时间参数 timeout，用来控制poll()方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率，可以把timeout设置为Long.MAX_VALUE;消费者消费到的每条消息的类型为 ConsumerRecordtopic partition 这两个字段分别代表消息所属主题的名称和所在分区的编号offset 表示消息在所属分区的偏移量timestamp 表示时间戳，与此对应的timestampType表示时间戳的类型timestampType 有两种类型CreateTime和LogAppendTime，分别代表消息创建的时间戳和消息追加到日志的时间戳headers 表示消息的头部内容key value 分别表示消息的键和消息的值,一般业务应用要读取的就是valueserializedKeySize、serializedValueSize 分别表示 key、value 经过序列化之后的大小，如果 key 为空, 则 serializedKeySize 值为-1，同样，如果value 为空，则serializedValueSize的值也会为-1checksum 是 CRC32 的校验值</code></pre></blockquote></blockquote><p>7、指定位移消费</p><blockquote><blockquote><blockquote></blockquote><pre><code>有些时候，我们需要一种更细粒度的掌控，可以让我们从特定的位移处开始拉取消息，而KafkaConsumer中的seek()方法正好提供了这个功能，让我们可以追前消费或回溯消费。seek()方法的具体定义如下: public void seek(TopicPartiton partition,long offset)</code></pre></blockquote></blockquote><p>8、再均衡监听器</p><blockquote><blockquote><blockquote></blockquote><pre><code>一个消费组中，一旦有消费者的增减发生，会触发消费者组的 rebalance 再均衡；如果 A 消费者消费掉的一批消息还没来得及提交 offset，而它所负责的分区在 rebalance 中转移给了B消费者，则有可能发生数据的重复消费处理。此情形下，可以通过再均衡监听器做一定程度的补救；</code></pre></blockquote></blockquote><p>9、自动位移提交</p><blockquote><blockquote><blockquote></blockquote><pre><code>Kafka中默认的消费位移的提交方式是自动提交，这个由消费者客户端参数enable.auto.commit 配置，默认值为 true 。当然这个默认的自动提交不是每消费一条消息就提交一次，而是定期提交，这个定期的周期时间由客户端参数 auto.commit.interval.ms 配置，默认值为 5 秒，此参数生效的前提是 enable.auto.commit 参数为 true。在默认的方式下，消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在poll()方法的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移。Kafka消费的编程逻辑中位移提交是一大难点，自动提交消费位移的方式非常简便，它免去了复杂的位移提交逻辑，让编码更简洁。但随之而来的是重复消费和消息丢失的问题。</code></pre></blockquote></blockquote><pre><code>重复消费   &gt;&gt;&gt; 假设刚刚提交完一次消费位移，然后拉取一批消息进行消费，在下一次自动提交消费位移之前，消费者崩溃了，那么又得从上一次位移提交的地方重新开始消费，这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小，但这样并不能避免重复消费的发送，而且也会使位移提交更加频繁。丢失消息   &gt;&gt;&gt; 按照一般思维逻辑而言，自动提交是延时提交，重复消费可以理解，那么消息丢失又是在什么情形下会发生的呢？我们来看下图中的情形：拉取线程不断地拉取消息并存入本地缓存，比如在 BlockingQueue中，另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第 y+l 次拉取，以及第 m 次位移提交的时候，也就是x+6 之前的位移己经确认提交了，处理线程却还正在处理 x+3 的消息；此时如果处理线程发生了异常，待其恢复之后会从第 m 次位移提交处，也就是 x+6 的位置开始拉取消息，那么 x+3 至 x+6 之间的消息就没有得到相应的处理，这样便发生消息丢失的现象。</code></pre><p>10、手动位移提交</p><blockquote><blockquote><blockquote></blockquote><pre><code>自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象，但是在编程的世界里异常无可避免；同时，自动位移提交也无法做到精确的位移管理。在 Kafka 中还提供了手动位移提交的方式，这样可以使得开发人员对消费位移的管理控制更加灵活。很多时候并不是说拉取到消息就算消费完成，而是需要将消息写入数据库、写入本地缓存，或者是更加复杂的业务处理。在这些场景下，所有的业务处理完成才能认为消息被成功消费； 手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。 开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 false，示例如下props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, false); 手动提交可以细分为同步提交和异步提交，对应于 KafkaConsumer 中的 commitSync()和commitAsync()两种类型的方法。同步提交的方式：对于采用 commitSync()的无参方法，它提交消费位移的频率和拉取批次消息、处理批次消息的频率是一样的，如果想寻求更细粒度的、更精准的提交，那么就需要使用 commitSync()的另一个有参方法，具体定义如下：public void commitSync(final Map&lt;TopicPartition,OffsetAndMetadata&gt; offsets)异步提交方式：commitSync()方法相反，异步提交的方式( commitAsync())在执行的时候消费者线程不会被阻塞；可能在提交消费位移的结果还未返回之前就开始了新一次的拉取操。异步提交以便消费者的性能得到一定的增强。</code></pre></blockquote></blockquote><p>11、其他重要参数</p><blockquote><blockquote><blockquote></blockquote><pre><code>fetch.min.bytes=1B     -&gt; 一次拉取的最小字节数fetch.max.bytes=50M -&gt; 一次拉取的最大数据量fetch.max.wait.ms=500ms -&gt;     拉取时的最大等待时长max.partition.fetch.bytes = 1MB -&gt; 每个分区一次拉取的最大数据量max.poll.records=500-&gt; 一次拉取的最大条数connections.max.idle.ms=540000ms -&gt; 网络连接的最大闲置时长request.timeout.ms=30000ms  -&gt; 一次请求等待响应的最大超时时间consumer 等待请求响应的最长时间metadata.max.age.ms=300000 -&gt; 元数据在限定时间内没有进行更新,则会被强制更新reconnect.backoff.ms=50ms     -&gt; 尝试重新连接指定主机之前的退避时间retry.backoff.ms=100ms -&gt; 尝试重新拉取数据的重试间隔</code></pre></blockquote></blockquote><p>12、新建ConsumerDemo、ConsumerDemo1、ConsumerTask、ConsumerDemo2、ConsumerSeekOffset类<br><img src="../images/8.png"><br><img src="../images/9.png"><br><img src="../images/10.png"><br><img src="../images/11.png"><br><img src="../images/12.png"></p><p>五、Topic管理API<br>1、一般情况下，我们都习惯使用 kafka-topic.sh 本来管理主题，如果希望将管理类的功能集成到公司内部的系统中，打造集管理、监控、运维、告警为一体的生态平台，那么就需要以程序调用 API 方式去实现。这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类。</p><blockquote><blockquote><blockquote></blockquote><pre><code>KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法：</code></pre><p>   创建主题：CreateTopicResult createTopics(Collection<New Topic> new Topics)<br>   删除主题：DeleteTopicsResult deleteTopics(Collection<String>topics)<br>   列出所有可用的主题：ListTopicsResult listTopics()<br>   查看主题的信息：DescribeTopicsResult describeTopics(Collection<String> topicNames)</p></blockquote></blockquote><p>查询配置信息：</p><blockquote><blockquote><blockquote></blockquote><p>   DescribeConfigsResult describeConfigs(Collection<ConfigResource>resources)<br>   修改配置信息：AlterConfigsResult alterConfigs(Map&lt;ConfigResource,Config&gt; configs)<br>   增加分区：CreatePartitionsResult createPartitions(Map&lt;String,NewPartitions&gt; new Partitions)<br>   构造一个 KafkaAdminClient AdminClient adminClient =<br>   KafkaAdminClient.create(props);</p></blockquote></blockquote><p>2、列出主题</p><blockquote><blockquote><blockquote></blockquote><p>   ListTopicsResult listTopicsResult = adminClient.listTopics();<br>   Set<String> topics = listTopicsResult.names().get();<br>   System.out.println(topics);</p></blockquote></blockquote><p>3、查看主题信息</p><blockquote><blockquote><blockquote></blockquote><p>   DescribeTopicsResult describeTopicsResult =<br>    adminClient.describeTopics(Arrays.asList(“tpc_4”, “tpc_3”));<br>   Map&lt;String, TopicDescription&gt; res = describeTopicsResult.all().get();<br>   Set<String> ksets = res.keySet();<br>   for (String k : ksets) {<br>       System.out.println(res.get(k));<br>   }</p></blockquote></blockquote><p>4、创建主题</p><blockquote><blockquote><blockquote></blockquote><p>   // 参数配置<br>   Properties props = new Properties();<br>   props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,”node1:9092,node2:9092,node3:9092”);<br>   props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,3000);<br>   // 创建 admin client 对象<br>   AdminClient adminClient = KafkaAdminClient.create(props);<br>   // 由服务端 controller 自行分配分区及副本所在 broker<br>   NewTopic tpc_3 = new NewTopic(“tpc_3”, 2, (short) 1);<br>   // 手动指定分区及副本的 broker 分配<br>   HashMap&lt;Integer, List<Integer>&gt; replicaAssignments = new HashMap&lt;&gt;();<br>   // 分区 0,分配到 broker0,broker1<br>    replicaAssignments.put(0,Arrays.asList(0,1));<br>   // 分区1,分配到 broker0,broker2<br>   replicaAssignments.put(0,Arrays.asList(0,1));<br>   NewTopic tpc_4 = new NewTopic(“tpc_4”, replicaAssignments);<br>   CreateTopicsResult result =<br>    adminClient.createTopics(Arrays.asList(tpc_3,tpc_4));<br>   // 从 future 中等待服务端返回<br>   try {<br>       result.all().get();<br>   } catch (Exception e) {<br>   e.printStackTrace();<br>   }<br>   adminClient.close();</p></blockquote></blockquote><p>5、删除主题 </p><blockquote><blockquote><blockquote></blockquote><p>   DeleteTopicsResult deleteTopicsResult =<br>    adminClient.deleteTopics(Arrays.asList(“tpc_1”, “tpc_1”));<br>   Map&lt;String, KafkaFuture<Void>&gt; values = deleteTopicsResult.values();<br>   System.out.println(values);</p></blockquote></blockquote><p>6、其他管理</p><blockquote><blockquote><blockquote></blockquote><p>   除了进行 topic 管理之外，KafkaAdminClient 也可以进行诸如动态参数管理,分区管理等各类管理操作；</p></blockquote></blockquote><p>7、新建KafkaAdminDemo、CallableDemo类<br><img src="../images/13.png"><br><img src="../images/14.png"><br>**<summary></p><p>**</p></details><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;kafka-API使用方法&quot;&gt;&lt;a href=&quot;#kafka-API使用方法&quot; class=&quot;headerlink&quot; title=&quot;kafka API使用方法&quot;&gt;&lt;/a&gt;kafka API使用方法&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;title: kafka API使用方法&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Kafka命令行操作</title>
    <link href="http://example.com/2022/06/06/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/"/>
    <id>http://example.com/2022/06/06/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/</id>
    <published>2022-06-06T02:00:00.000Z</published>
    <updated>2022-06-19T10:21:15.497Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kafka命令行操作"><a href="#Kafka命令行操作" class="headerlink" title="Kafka命令行操作"></a>Kafka命令行操作</h1><hr><p>title: Kafka命令行操作</p><details><summary>阅读全文</summary><h1 id="二、Kafka命令行操作"><a href="#二、Kafka命令行操作" class="headerlink" title="二、Kafka命令行操作"></a>二、Kafka命令行操作</h1><h3 id="1、cd-export-server-kafka-bin目录下的命令行工具"><a href="#1、cd-export-server-kafka-bin目录下的命令行工具" class="headerlink" title="1、cd /export/server/kafka/bin目录下的命令行工具"></a>1、cd /export/server/kafka/bin目录下的命令行工具</h3><blockquote><p>kafka-configs.sh    用于配置管理<br>kafka-console-consumer.sh    用于消费消息<br>kafka-console-producer.sh    用于生产消息<br>kafka-consumer-perf-test.sh    用于测试消费性能<br>kafka-topics.sh    用于管理主题<br>kafka-dump-log.sh    用于查看日志内容<br>kafka-server-stop.sh    用于关闭kafka服务<br>kafka-preferred-replica-election.sh    用于优先副本的选举<br>kafka-server-start.sh    用于启动kafka服务<br>kafka-producer-perf-test.sh    用于测试生产性能<br>kafka-reassign-partitions.sh    用于分区重分配</p></blockquote><h3 id="2、查看当前可用topic（应该什么也没有）"><a href="#2、查看当前可用topic（应该什么也没有）" class="headerlink" title="2、查看当前可用topic（应该什么也没有）"></a>2、查看当前可用topic（应该什么也没有）</h3><blockquote><p>(base) [root@node1 bin]# kafka-topics.sh –list –zookeeper node1:2181<br>__consumer_offsets<br>test12<br>test3<br>test6<br>test9<br>tpc_1<br>tpc_10<br>tpc_2<br>tpc_4</p></blockquote><h3 id="3、创建topic，检查是否创建成功"><a href="#3、创建topic，检查是否创建成功" class="headerlink" title="3、创建topic，检查是否创建成功"></a>3、创建topic，检查是否创建成功</h3><blockquote><p>(base) [root@node1 bin]# kafka-topics.sh –create –topic tpc_5 –partitions 2 –replication-factor 2 –zookeeper node1:2181<br>WARNING: Due to limitations in metric names, topics with a period (‘.’) or underscore (‘_’) could collide. To avoid issues it is best to use either, but not both.<br>Created topic “tpc_5”.<br>(base) [root@node1 bin]# kafka-topics.sh –list –zookeeper node1:2181<br>__consumer_offsets<br>test12<br>test3<br>test6<br>test9<br>tpc_1<br>tpc_10<br>tpc_2<br>tpc_4<br>tpc_5</p></blockquote><h3 id="4、手动指定副本的存储位置，进入cd-export-data-kafka-logs路径下查看分区的分布"><a href="#4、手动指定副本的存储位置，进入cd-export-data-kafka-logs路径下查看分区的分布" class="headerlink" title="4、手动指定副本的存储位置，进入cd /export/data/kafka-logs路径下查看分区的分布"></a>4、手动指定副本的存储位置，进入cd /export/data/kafka-logs路径下查看分区的分布</h3><blockquote><p>(base) [root@node1 bin]# kafka-topics.sh –create –topic tpc_6  –zookeeper node1:2181 –replica-assignment 0:1,1:2<br>WARNING: Due to limitations in metric names, topics with a period (‘.’) or underscore (‘_’) could collide. To avoid issues it is best to use either, but not both.<br>Created topic “tpc_6”.</p></blockquote><blockquote><p>(base) [root@node1 bin]# ll /export/data/kafka-logs<br>总用量 32<br>-rw-r–r– 1 root root  181 5月  13 13:42 cleaner-offset-checkpoint<br>drwxr-xr-x 2 root root  319 6月  19 15:34 __consumer_offsets-1<br>drwxr-xr-x 2 root root  141 3月  25 15:02 __consumer_offsets-10<br>drwxr-xr-x 2 root root  319 6月  19 15:34 __consumer_offsets-13<br>drwxr-xr-x 2 root root  141 3月  25 15:02 __consumer_offsets-16<br>drwxr-xr-x 2 root root  141 3月  25 15:02 __consumer_offsets-19<br>drwxr-xr-x 2 root root  141 3月  25 15:02 __consumer_offsets-22<br>drwxr-xr-x 2 root root  178 6月  19 15:34 __consumer_offsets-25<br>drwxr-xr-x 2 root root  141 3月  25 15:02 __consumer_offsets-28<br>drwxr-xr-x 2 root root  141 3月  25 15:02 __consumer_offsets-31<br>drwxr-xr-x 2 root root  141 3月  25 15:02 __consumer_offsets-34<br>drwxr-xr-x 2 root root 4096 6月  19 15:34 __consumer_offsets-37<br>drwxr-xr-x 2 root root  141 3月  25 15:02 __consumer_offsets-4<br>drwxr-xr-x 2 root root 4096 6月  19 15:34 __consumer_offsets-40<br>drwxr-xr-x 2 root root 4096 6月  19 15:34 __consumer_offsets-43<br>drwxr-xr-x 2 root root  141 3月  25 15:02 __consumer_offsets-46<br>drwxr-xr-x 2 root root  319 6月  19 15:34 __consumer_offsets-49<br>drwxr-xr-x 2 root root  319 6月  19 15:34 __consumer_offsets-7<br>-rw-r–r– 1 root root    4 6月  19 15:43 log-start-offset-checkpoint<br>-rw-r–r– 1 root root   54 3月  23 15:01 meta.properties<br>-rw-r–r– 1 root root  807 6月  19 15:43 recovery-point-offset-checkpoint<br>-rw-r–r– 1 root root  797 6月  19 15:44 replication-offset-checkpoint<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test12-0<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test12-1<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test12-10<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test12-11<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test12-4<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test12-5<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test12-6<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test12-7<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test3-0<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test3-2<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test6-1<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test6-2<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test6-3<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test6-5<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test9-0<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test9-2<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test9-3<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test9-4<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test9-6<br>drwxr-xr-x 2 root root  141 6月  19 15:34 test9-8<br>drwxr-xr-x 2 root root  141 4月  29 13:48 tpc_10-0<br>drwxr-xr-x 2 root root  141 4月  29 13:48 tpc_10-2<br>drwxr-xr-x 2 root root  178 6月  19 15:34 tpc_1-1<br>drwxr-xr-x 2 root root  178 6月  19 15:34 tpc_1-2<br>drwxr-xr-x 2 root root  141 6月  19 15:34 tpc_2-1<br>drwxr-xr-x 2 root root  141 6月  19 15:34 tpc_4-0<br>drwxr-xr-x 2 root root  141 6月  19 15:34 tpc_4-1<br>drwxr-xr-x 2 root root  141 6月  19 15:37 tpc_5-0<br>drwxr-xr-x 2 root root  141 6月  19 15:42 tpc_6-0</p></blockquote><h3 id="5、进入到zookeeper-client，查看目录、controller、controller-epoch"><a href="#5、进入到zookeeper-client，查看目录、controller、controller-epoch" class="headerlink" title="5、进入到zookeeper client，查看目录、controller、controller_epoch"></a>5、进入到zookeeper client，查看目录、controller、controller_epoch</h3><blockquote><p>WatchedEvent state:SyncConnected type:None path:null<br>[zk：localhost:2181(CONNECTED) 0] ls /<br>[admin, brokers, cluster, config, consumers, controller, controller epoch, isr change notification, latest producer id block, log dir _event notification, spark-ha, zookee per]<br>[zk：localhost:2181(CONNECTED) 1] get/controller<br>(“version”:1,”brokerid”:0, “timestamp”:”1651629958728”)<br>[zk: localhost:2181(CONNECTED) 2] get /controller_epoch<br>31</p></blockquote><h3 id="6、删除topic，检查是否删除成功"><a href="#6、删除topic，检查是否删除成功" class="headerlink" title="6、删除topic，检查是否删除成功"></a>6、删除topic，检查是否删除成功</h3><blockquote><p>(base) [root@node1 bin]# kafka-topics.sh  –delete –topic tpc_6 –zookeeper node1:2181<br>Topic tpc_6 is marked for deletion.<br>Note: This will have no impact if delete.topic.enable is not set to true.</p></blockquote><h3 id="7、查看topic详情"><a href="#7、查看topic详情" class="headerlink" title="7、查看topic详情"></a>7、查看topic详情</h3><blockquote><p>(base) [root@node1 bin]#kafka-topics.sh –describe –topic tpc_4 –zookeeper nodel:2181<br>Topic:tpc 4 PartitionCount:2 ReplicationFactor:2 Confias:<br>Topic: tpc 4 Partition:0 Leader: 0 Replicas: 0,1 Isr:0,1<br>Topic：tpc_4 Partition：1 Leader:1 Replicas:1,2 Isr:1,2</p></blockquote><h3 id="8、增加分区数，查看topic详情"><a href="#8、增加分区数，查看topic详情" class="headerlink" title="8、增加分区数，查看topic详情"></a>8、增加分区数，查看topic详情</h3><blockquote><p>(base)[root@node1 bin]#kafka-topics.sh–alter topic tpc 4 -partitions 3 –zookeeper node1:2181<br>WARNING: If partitions are increased for a topic that has a key, the partition logic or orderin g of the messages will be affected<br>Adding partitions succeeded!<br>(base) [root@node1 bin]#kafka-topics.sh –describe –topic tpc 4 –zookeeper node1:2181Topic:tpc_4   PartitionCount:3 ReplicationFactor:2 Configs:<br>Topic: tpc_4 Partition:0 Leader:0 Replicas：0,1 Isr：0,1<br>Topic：tpc_4 Partition:1 Leader：1 Replicas：1,2 Isr：1,2<br>Topic: tpc 4 Partition:2 Leader:2 Replicas:2,0 Isr: 2,0</p></blockquote><h3 id="9、修改参数"><a href="#9、修改参数" class="headerlink" title="9、修改参数"></a>9、修改参数</h3><blockquote><p>(base)[root@node1 ~]#kafka-configs.sh –zookeeper nodel:2181 –entity-type topics –entity-na me tpc 4 –alter -add-config compression.type=gzip<br>Completed Updating config for entity: topic ‘tpc_4’.<br>(base) [rootenode1 ~]# kafka-topics.sh –describe –topic tpc 4–zookeeper node1:2181<br>Topic:tpc 4 PartitionCount:3 ReplicationFactor:2 Configs:compression.type=gzip<br>Topic：tpc_4 partition：0 Leader：0 Replicas:0,1 Isr:0,1<br>Topic： tpc 4 Partition：1 Leader：1 Replicas：1,2 Isr：2,1<br>Topic：tpc 4 Partition：2 Leader:0 Replicas：2,0 Isr：0,2<br>您在 /var/spool/mail/root 中有新邮件<br>(base) [rootenode1 ~]# kafka-configs.sh –zookeeper node1:2181–entity-type topics –entity-na me tpc_4 –alter –delete-config compression.type<br>Completed Updating config for entity: topic ‘tpc_4’.<br>您在 /var/spool/mail/root 中有新邮件<br>(base) [root@node1 ~]#kafka-topics.sh –describe –topic tpc_4 –zookeeper nodel:2181<br>Topic:tpc 4 PartitionCount:3 ReplicationFactor:2 Configs:<br>Topic：tpc_4 partition：0 Leader：0 Replicas：0,1 Isr:0,1<br>Topic：tpc_4 Partition：1 Leader:1 Replicas：1,2 Isr:2,1<br>Topic：tpc_4 Partition：2 Leader：0 Replicas：2,0 Isr: 0,2</p></blockquote><h3 id="10、生产者写入数据、消费者拉取数据"><a href="#10、生产者写入数据、消费者拉取数据" class="headerlink" title="10、生产者写入数据、消费者拉取数据"></a>10、生产者写入数据、消费者拉取数据</h3><blockquote><p>zookeeper.node1:2181<br>(base)[root@node1 bin]#./kafka-console-producer.sh –broker-list node1:9092, node2:9092, node3:9092 –topic tpc_4  </p><blockquote><p>helloword<br>kafka<br>nihao</p></blockquote></blockquote><blockquote><p>(base) [root@node2 bin]#kafka-console-consumer.sh –bootstrap-server nodel:9092, node2:9092, node1:9092 –topic tpc_4 –from-beginning<br>helloword<br>kafka<br>nihao</p></blockquote><p>**</p></details><hr>]]></content>
    
    
    <summary type="html">Kafka命令行操作</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Kafka环境配置</title>
    <link href="http://example.com/2022/06/02/Kafka%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/2022/06/02/Kafka%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</id>
    <published>2022-06-02T10:00:00.000Z</published>
    <updated>2022-06-19T10:21:07.308Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kafka环境配置"><a href="#Kafka环境配置" class="headerlink" title="Kafka环境配置"></a>Kafka环境配置</h1><hr><p>title: Kafka环境配置</p><details><summary>阅读全文</summary><h1 id="一、Kafka环境配置"><a href="#一、Kafka环境配置" class="headerlink" title="一、Kafka环境配置"></a>一、Kafka环境配置</h1><h3 id="1、上传文件包到-export-server-，解压文件"><a href="#1、上传文件包到-export-server-，解压文件" class="headerlink" title="1、上传文件包到/export/server/ ，解压文件"></a>1、上传文件包到/export/server/ ，解压文件</h3><blockquote><p>[root@node1 ~]# tar -zxvf kafka_2.11-2.0.0.tgz</p></blockquote><h3 id="2、创建软连接"><a href="#2、创建软连接" class="headerlink" title="2、创建软连接"></a>2、创建软连接</h3><blockquote><p>[root@node1 ~]# ln -s kafka_2.11-2.0.0/ kafka</p></blockquote><h3 id="3、进入-export-server-kafka-confifig修改配置文件server-properties"><a href="#3、进入-export-server-kafka-confifig修改配置文件server-properties" class="headerlink" title="3、进入 /export/server/kafka/confifig修改配置文件server.properties"></a>3、进入 /export/server/kafka/confifig修改配置文件server.properties</h3><blockquote><p>[root@node1 config]# cd /export/server/kafka/config<br>[root@node1 config]# vim server.properties</p></blockquote><h4 id="（1）、21-行内容broker-id-0为依次增长的-0、1、2、3、4-集群中唯一-id从0开始，每台不能重复"><a href="#（1）、21-行内容broker-id-0为依次增长的-0、1、2、3、4-集群中唯一-id从0开始，每台不能重复" class="headerlink" title="（1）、21 行内容broker.id=0为依次增长的:0、1、2、3、4,集群中唯一 id从0开始，每台不能重复"></a>（1）、21 行内容broker.id=0为依次增长的:0、1、2、3、4,集群中唯一 id从0开始，每台不能重复</h4><blockquote><p>############################# Server Basics #############################<br>#The id of the broker. This must be set to a unique integer for each broker.<br>broker.id=0</p></blockquote><h4 id="（2）、31-行内容-listeners-PLAINTEXT-9092-取消注释，内容改为：-listeners-PLAINTEXT-node1-9092-PLAINTEXT为通信使用明文（加密ssl）"><a href="#（2）、31-行内容-listeners-PLAINTEXT-9092-取消注释，内容改为：-listeners-PLAINTEXT-node1-9092-PLAINTEXT为通信使用明文（加密ssl）" class="headerlink" title="（2）、31 行内容 #listeners=PLAINTEXT://:9092 取消注释，内容改为： listeners=PLAINTEXT://node1:9092 PLAINTEXT为通信使用明文（加密ssl）"></a>（2）、31 行内容 #listeners=PLAINTEXT://:9092 取消注释，内容改为： listeners=PLAINTEXT://node1:9092 PLAINTEXT为通信使用明文（加密ssl）</h4><blockquote><p>############################# Socket Server Settings #############################  </p></blockquote><blockquote><p><algorithm># The address the socket server listens on. It will get the value returned from<br><algorithm># java.net.InetAddress.getCanonicalHostName() if not configured.<br><algorithm>#   FORMAT:<br><algorithm>#     listeners = listener_name://host_name:port<br><algorithm>#   EXAMPLE:<br><algorithm>#     listeners = PLAINTEXT://your.host.name:9092<br>#listeners=PLAINTEXT://:9092<br>listeners=PLAINTEXT://192.168.88.151:9092</p></blockquote><h4 id="（3）、59-行内容-log-dirs-tmp-kafka-logs-为默认日志文件存储的位置，改为-log-dirs-export-server-data-kafka-logs"><a href="#（3）、59-行内容-log-dirs-tmp-kafka-logs-为默认日志文件存储的位置，改为-log-dirs-export-server-data-kafka-logs" class="headerlink" title="（3）、59 行内容 log.dirs=/tmp/kafka-logs 为默认日志文件存储的位置，改为 log.dirs=/export/server/data/kafka-logs"></a>（3）、59 行内容 log.dirs=/tmp/kafka-logs 为默认日志文件存储的位置，改为 log.dirs=/export/server/data/kafka-logs</h4><blockquote><p>############################# Log Basics #############################  </p></blockquote><blockquote><p><algorithm># A comma separated list of directories under which to store log files<br>log.dirs=/export/data/kafka-logs</p></blockquote><h4 id="（4）、63-行内容为-num-partitions-1-是默认分区数"><a href="#（4）、63-行内容为-num-partitions-1-是默认分区数" class="headerlink" title="（4）、63 行内容为 num.partitions=1 是默认分区数"></a>（4）、63 行内容为 num.partitions=1 是默认分区数</h4><blockquote><p>num.partitions=1</p></blockquote><h4 id="（5）、76-行部分数据安全性（持久化之前先放到缓存上，从缓存刷到磁盘上interval-messages-interval-ms"><a href="#（5）、76-行部分数据安全性（持久化之前先放到缓存上，从缓存刷到磁盘上interval-messages-interval-ms" class="headerlink" title="（5）、76 行部分数据安全性（持久化之前先放到缓存上，从缓存刷到磁盘上interval.messages interval.ms)"></a>（5）、76 行部分数据安全性（持久化之前先放到缓存上，从缓存刷到磁盘上interval.messages interval.ms)</h4><blockquote><p><algorithm># The number of messages to accept before forcing a flush of data to disk<br>#log.flush.interval.messages=10000</p></blockquote><h4 id="（6）、93-行部分（数据保留策略-168-24-7，1073741824-1024-1GB，300000ms-300s-5min超过了删掉（最后修改时间还是创建时间–-gt-日志段中最晚的一条消息，维护这个最大的时间戳–-gt-用户无法干预）"><a href="#（6）、93-行部分（数据保留策略-168-24-7，1073741824-1024-1GB，300000ms-300s-5min超过了删掉（最后修改时间还是创建时间–-gt-日志段中最晚的一条消息，维护这个最大的时间戳–-gt-用户无法干预）" class="headerlink" title="（6）、93 行部分（数据保留策略 168/24=7，1073741824/1024=1GB，300000ms = 300s = 5min超过了删掉（最后修改时间还是创建时间–&gt;日志段中最晚的一条消息，维护这个最大的时间戳–&gt;用户无法干预）"></a>（6）、93 行部分（数据保留策略 168/24=7，1073741824/1024=1GB，300000ms = 300s = 5min超过了删掉（最后修改时间还是创建时间–&gt;日志段中最晚的一条消息，维护这个最大的时间戳–&gt;用户无法干预）</h4><blockquote><h1 id="The-minimum-age-of-a-log-file-to-be-eligible-for-deletion-due-to-age"><a href="#The-minimum-age-of-a-log-file-to-be-eligible-for-deletion-due-to-age" class="headerlink" title="The minimum age of a log file to be eligible for deletion due to age"></a>The minimum age of a log file to be eligible for deletion due to age</h1><p>log.retention.hours=168</p></blockquote><h4 id="（7）、121-行内容-zookeeper-connect-localhost-2181-修改为-zookeeper-connect-node1-2181-node2-2181，node3-2181"><a href="#（7）、121-行内容-zookeeper-connect-localhost-2181-修改为-zookeeper-connect-node1-2181-node2-2181，node3-2181" class="headerlink" title="（7）、121 行内容 zookeeper.connect=localhost:2181 修改为 zookeeper.connect=node1:2181,node2:2181，node3:2181"></a>（7）、121 行内容 zookeeper.connect=localhost:2181 修改为 zookeeper.connect=node1:2181,node2:2181，node3:2181</h4><blockquote><p>############################# Zookeeper #############################  </p><p><algorithm># Zookeeper connection string (see zookeeper docs for details).<br><algorithm># This is a comma separated host:port pairs, each corresponding to a zk<br><algorithm># server. e.g. “127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002”.<br><algorithm># You can also append an optional chroot string to the urls to specify the<br><algorithm># root directory for all kafka znodes.<br>zookeeper.connect=node1:2181,node2:2181,node3:2181<br><algorithm># Timeout in ms for connecting to zookeeper<br>zookeeper.connection.timeout.ms=6000</p></blockquote><h4 id="（8）、126-行内容-group-initial-rebalance-delay-ms-0-修改为-group-initial-rebalance-delay-ms-3000"><a href="#（8）、126-行内容-group-initial-rebalance-delay-ms-0-修改为-group-initial-rebalance-delay-ms-3000" class="headerlink" title="（8）、126 行内容 group.initial.rebalance.delay.ms=0 修改为 group.initial.rebalance.delay.ms=3000"></a>（8）、126 行内容 group.initial.rebalance.delay.ms=0 修改为 group.initial.rebalance.delay.ms=3000</h4><blockquote><p>############################# Group Coordinator Settings #############################  </p><p><algorithm># The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.<br><algorithm># The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.<br><algorithm># The default value for this is 3 seconds.<br><algorithm># We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.<br><algorithm># However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.<br>group.initial.rebalance.delay.ms=3000</p></blockquote><h3 id="4、给-node2和-node3-scp-分发-kafka"><a href="#4、给-node2和-node3-scp-分发-kafka" class="headerlink" title="4、给 node2和 node3 scp 分发 kafka"></a>4、给 node2和 node3 scp 分发 kafka</h3><h3 id="5、创建软连接"><a href="#5、创建软连接" class="headerlink" title="5、创建软连接"></a>5、创建软连接</h3><h3 id="6、配置-kafka-环境变量（注：可以一台一台配，也可以在-node1-完成后发给-node21-和node2）"><a href="#6、配置-kafka-环境变量（注：可以一台一台配，也可以在-node1-完成后发给-node21-和node2）" class="headerlink" title="6、配置 kafka 环境变量（注：可以一台一台配，也可以在 node1 完成后发给 node21 和node2）"></a>6、配置 kafka 环境变量（注：可以一台一台配，也可以在 node1 完成后发给 node21 和node2）</h3><blockquote><p>#ZOOKEEPER_HOME<br>export ZOOKEEPER_HOME=/export/server/zookeeper<br>export PATH=$PATH:$ZOOKEEPER_HOME/bin<br>#KAFKA_HOME<br>export KAFKA_HOME=/export/server/kafka<br>export PATH=$PATH:$KAFKA_HOME/bin</p></blockquote><h3 id="7、重新加载环境变量"><a href="#7、重新加载环境变量" class="headerlink" title="7、重新加载环境变量"></a>7、重新加载环境变量</h3><h3 id="8、分别在node2-和node3-上修改配置文件："><a href="#8、分别在node2-和node3-上修改配置文件：" class="headerlink" title="8、分别在node2 和node3 上修改配置文件："></a>8、分别在node2 和node3 上修改配置文件：</h3><h4 id="（1）、将server-properties-的第-21-行的-broker-id-0-修改为-broker-id-1-同理-node3-同样操作"><a href="#（1）、将server-properties-的第-21-行的-broker-id-0-修改为-broker-id-1-同理-node3-同样操作" class="headerlink" title="（1）、将server.properties 的第 21 行的 broker.id=0 修改为 broker.id=1 同理 node3 同样操作"></a>（1）、将server.properties 的第 21 行的 broker.id=0 修改为 broker.id=1 同理 node3 同样操作</h4><blockquote><p>############################# Server Basics #############################<br>#The id of the broker. This must be set to a unique integer for each broker.<br>broker.id=1</p></blockquote><h4 id="（2）、将文件-server-properties-的第-31-行的-listeners-PLAINTEXT-node1-9092-修改为-listeners-PLAINTEXT-node2-9092-同理node3-同样操作"><a href="#（2）、将文件-server-properties-的第-31-行的-listeners-PLAINTEXT-node1-9092-修改为-listeners-PLAINTEXT-node2-9092-同理node3-同样操作" class="headerlink" title="（2）、将文件 server.properties 的第 31 行的 listeners=PLAINTEXT://node1:9092 修改为 listeners=PLAINTEXT://node2:9092 同理node3 同样操作"></a>（2）、将文件 server.properties 的第 31 行的 listeners=PLAINTEXT://node1:9092 修改为 listeners=PLAINTEXT://node2:9092 同理node3 同样操作</h4><blockquote><p>listeners=PLAINTEXT://192.168.88.152:9092</p></blockquote><h3 id="9、启停-kafka-注：kafka-启动需要在-zookeeper-启动的情况下才可-kafka-server-start-sh-daemon-export-server-kafka-config-server-properties-hadoop，zookeeper，kafka启动"><a href="#9、启停-kafka-注：kafka-启动需要在-zookeeper-启动的情况下才可-kafka-server-start-sh-daemon-export-server-kafka-config-server-properties-hadoop，zookeeper，kafka启动" class="headerlink" title="9、启停 kafka (注：kafka 启动需要在 zookeeper 启动的情况下才可) kafka-server-start.sh -daemon /export/server/kafka/config/server.properties hadoop，zookeeper，kafka启动"></a>9、启停 kafka (注：kafka 启动需要在 zookeeper 启动的情况下才可) kafka-server-start.sh -daemon /export/server/kafka/config/server.properties hadoop，zookeeper，kafka启动</h3><blockquote><p>[root@node1 ~]# jps<br>88113 DataNode<br>13457 QuorumPeerMain<br>87682 NameNode<br>89764 NodeManager<br>89319 ResourceManager<br>26486 Jps<br>91659 kafka</p></blockquote><blockquote><p>[root@node2 ~]# jps<br>19489 Jps<br>5992 QuorumPeerMain<br>84298 SecondaryNameNode<br>95959 kafka<br>84778 NodeManager<br>83724 DataNode</p></blockquote><blockquote><p>[root@node3 ~]# jps<br>5412 QuorumPeerMain<br>19516 Jps<br>84524 NodeManager<br>95204 kafka<br>83566 DataNode</p></blockquote><h3 id="10、关闭-kafka"><a href="#10、关闭-kafka" class="headerlink" title="10、关闭 kafka"></a>10、关闭 kafka</h3><blockquote><p>[root@node2 kafka]# kafka-server-stop.sh stop</p></blockquote><h3 id="11、定制脚本一键启动-vim-kafka-all-sh-放入-bin-路径下"><a href="#11、定制脚本一键启动-vim-kafka-all-sh-放入-bin-路径下" class="headerlink" title="11、定制脚本一键启动 vim kafka-all.sh 放入 /bin 路径下"></a>11、定制脚本一键启动 vim kafka-all.sh 放入 /bin 路径下</h3><blockquote><p>#!/bin/bash<br>if [ $# -eq 0 ]<br>then<br>echo “please input param:start stop”<br>else<br>if [ $1 = start  ]<br>then<br>for i in {1..3}<br>do<br>echo “${1}ing node${i}”<br>ssh node${i} “source /etc/profile;/export/server/kafka/bin/kafka-server-start.sh -daemon /export/server/kafka/config/server.properties”<br>done<br>fi<br>if [ $1 = stop ]<br>then<br>for i in {1..3}<br>do<br>echo “${1}ing node${i}”<br>ssh node${i} “source /etc/profile;/export/server/kafka/bin/kafka-server-stop.sh”<br>done<br>fi<br>fi</p></blockquote><p>**</p></details><hr>]]></content>
    
    
    <summary type="html">Kafka环境配置</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2022/05/28/hello-world/"/>
    <id>http://example.com/2022/05/28/hello-world/</id>
    <published>2022-05-28T10:50:51.021Z</published>
    <updated>2022-06-19T10:15:14.222Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Spark HA &amp; Yarn配置</title>
    <link href="http://example.com/2022/05/27/Spark%20HA%20&amp;%20Yarn%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/2022/05/27/Spark%20HA%20&amp;%20Yarn%E9%85%8D%E7%BD%AE/</id>
    <published>2022-05-27T12:00:00.000Z</published>
    <updated>2022-06-19T10:34:22.921Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark-HA-amp-Yarn配置"><a href="#Spark-HA-amp-Yarn配置" class="headerlink" title="Spark HA &amp; Yarn配置"></a>Spark HA &amp; Yarn配置</h1><hr><p>title: Spark HA &amp; Yarn配置<br>date: 2022-05-27 20:00:00<br>description: Spark HA &amp; Yarn配置</p><details><summary>阅读全文</summary><h1 id="七、Spark（HA）"><a href="#七、Spark（HA）" class="headerlink" title="七、Spark（HA）"></a>七、Spark（HA）</h1><h3 id="1、打开zookeeper和hdfs"><a href="#1、打开zookeeper和hdfs" class="headerlink" title="1、打开zookeeper和hdfs"></a>1、打开zookeeper和hdfs</h3><h3 id="2、修改cd-export-server-spark-conf-路径下的spark-env-sh"><a href="#2、修改cd-export-server-spark-conf-路径下的spark-env-sh" class="headerlink" title="2、修改cd /export/server/spark/conf/路径下的spark-env.sh"></a>2、修改cd /export/server/spark/conf/路径下的spark-env.sh</h3><blockquote><p><algorithm>## 指定spark老大Master的IP和提交任务的通信端口<br><algorithm># 告知Spark的master运行在哪个机器上<br>#export SPARK_MASTER_HOST=node1<br><algorithm># 告知sparkmaster的通讯端口<br>export SPARK_MASTER_PORT=7077<br><algorithm># 告知spark master的 webui端口<br>SPARK_MASTER_WEBUI_PORT=8080</p></blockquote><blockquote><p>SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha”<br><algorithm># spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现<br><algorithm># 指定Zookeeper的连接地址<br><algorithm># 指定在Zookeeper中注册临时节点的路径</p></blockquote><h3 id="3、使用scp拷贝spark-env-sh到node2-node3上"><a href="#3、使用scp拷贝spark-env-sh到node2-node3上" class="headerlink" title="3、使用scp拷贝spark-env.sh到node2,node3上"></a>3、使用scp拷贝spark-env.sh到node2,node3上</h3><h3 id="4、关闭standalone，开启standalone，查看node1、node2的8080端口"><a href="#4、关闭standalone，开启standalone，查看node1、node2的8080端口" class="headerlink" title="4、关闭standalone，开启standalone，查看node1、node2的8080端口"></a>4、关闭standalone，开启standalone，查看node1、node2的8080端口</h3><p><img src="../picture/HA%E5%89%8Dnode1%E7%9A%848080.png"><br><img src="../picture/HA%E5%89%8Dnode2%E7%9A%848080.png"></p><h3 id="5、更换zookeeper版本"><a href="#5、更换zookeeper版本" class="headerlink" title="5、更换zookeeper版本"></a>5、更换zookeeper版本</h3><h4 id="（1）、将zookeepe新版本压缩包上传到-export-server-路径下，解压后删除压缩包"><a href="#（1）、将zookeepe新版本压缩包上传到-export-server-路径下，解压后删除压缩包" class="headerlink" title="（1）、将zookeepe新版本压缩包上传到/export/server/路径下，解压后删除压缩包"></a>（1）、将zookeepe新版本压缩包上传到/export/server/路径下，解压后删除压缩包</h4><h4 id="（2）、在cd-export-server-apache-zookeeper-3-7-0-bin-conf-路径下进行文件配置"><a href="#（2）、在cd-export-server-apache-zookeeper-3-7-0-bin-conf-路径下进行文件配置" class="headerlink" title="（2）、在cd /export/server/apache-zookeeper-3.7.0-bin/conf/路径下进行文件配置"></a>（2）、在cd /export/server/apache-zookeeper-3.7.0-bin/conf/路径下进行文件配置</h4><h5 id="①将zoo-sample-cfg复制为zoo-cfg：cp-zoo-sample-cfg-zoo-cfg"><a href="#①将zoo-sample-cfg复制为zoo-cfg：cp-zoo-sample-cfg-zoo-cfg" class="headerlink" title="①将zoo_sample.cfg复制为zoo.cfg：cp zoo_sample.cfg zoo.cfg"></a>①将zoo_sample.cfg复制为zoo.cfg：cp zoo_sample.cfg zoo.cfg</h5><h5 id="②在-export-server-zookeeper-路径下新建zkdatas文件夹：mkdir-p-export-server-zookeeper-zkdatas"><a href="#②在-export-server-zookeeper-路径下新建zkdatas文件夹：mkdir-p-export-server-zookeeper-zkdatas" class="headerlink" title="②在/export/server/zookeeper/路径下新建zkdatas文件夹：mkdir -p /export/server/zookeeper/zkdatas/"></a>②在/export/server/zookeeper/路径下新建zkdatas文件夹：mkdir -p /export/server/zookeeper/zkdatas/</h5><h5 id="③编辑zoo-cfg"><a href="#③编辑zoo-cfg" class="headerlink" title="③编辑zoo.cfg"></a>③编辑zoo.cfg</h5><blockquote><p>dataDir=/export/server/zookeeper/zkdatas</p></blockquote><blockquote><p>#集群中服务器地址<br>server.1=node1:2888:3888<br>server.2=node2:2888:3888<br>server.3=node3:2888:3888</p></blockquote><h5 id="④添加node1的myid配置：echo-1-gt-export-server-zookeeper-zkdatas-myid"><a href="#④添加node1的myid配置：echo-1-gt-export-server-zookeeper-zkdatas-myid" class="headerlink" title="④添加node1的myid配置：echo 1 &gt; /export/server/zookeeper/zkdatas/myid"></a>④添加node1的myid配置：echo 1 &gt; /export/server/zookeeper/zkdatas/myid</h5><blockquote><p>(base) [root@node1 zkdatas]# cat myid<br>1</p></blockquote><h5 id="⑤使用scp拷贝新版本zookeeper到node2-node3上"><a href="#⑤使用scp拷贝新版本zookeeper到node2-node3上" class="headerlink" title="⑤使用scp拷贝新版本zookeeper到node2,node3上"></a>⑤使用scp拷贝新版本zookeeper到node2,node3上</h5><h5 id="⑥停止zookeeper：zkall-sh-stop或者-export-server-zookeeper-bin-zkServer-sh-stop"><a href="#⑥停止zookeeper：zkall-sh-stop或者-export-server-zookeeper-bin-zkServer-sh-stop" class="headerlink" title="⑥停止zookeeper：zkall.sh stop或者/export/server/zookeeper/bin/zkServer.sh stop"></a>⑥停止zookeeper：zkall.sh stop或者/export/server/zookeeper/bin/zkServer.sh stop</h5><blockquote><p>(base) [root@node1 ~]# zkall.sh stop<br>stopping node1<br>ZooKeeper JMX enabled by default<br>Using config: /export/server/zookeeper/bin/../conf/zoo.cfg<br>Stopping zookeeper … STOPPED<br>stopping node2<br>ZooKeeper JMX enabled by default<br>Using config: /export/server/zookeeper/bin/../conf/zoo.cfg<br>Stopping zookeeper … STOPPED<br>stopping node3<br>ZooKeeper JMX enabled by default<br>Using config: /export/server/zookeeper/bin/../conf/zoo.cfg<br>Stopping zookeeper … STOPPED</p></blockquote><h5 id="⑦删掉三台机器之前的zookeeper软连接：rm-rf-export-server-zookeeper"><a href="#⑦删掉三台机器之前的zookeeper软连接：rm-rf-export-server-zookeeper" class="headerlink" title="⑦删掉三台机器之前的zookeeper软连接：rm -rf /export/server/zookeeper"></a>⑦删掉三台机器之前的zookeeper软连接：rm -rf /export/server/zookeeper</h5><h5 id="⑧重新配置三台机器的zookeeper软连接：ln-s-export-server-apache-zookeeper-3-7-0-bin-export-server-zookeeper"><a href="#⑧重新配置三台机器的zookeeper软连接：ln-s-export-server-apache-zookeeper-3-7-0-bin-export-server-zookeeper" class="headerlink" title="⑧重新配置三台机器的zookeeper软连接：ln -s /export/server/apache-zookeeper-3.7.0-bin /export/server/zookeeper"></a>⑧重新配置三台机器的zookeeper软连接：ln -s /export/server/apache-zookeeper-3.7.0-bin /export/server/zookeeper</h5><blockquote><p>drwxr-xr-x  8 root  root        160 3月  29 17:05 apache-zookeeper-3.7.0-bin<br>drwxr-xr-x  3 root  root         24 3月  18 16:42 data<br>lrwxrwxrwx  1 root  root         28 3月  11 14:51 hadoop -&gt; /export/server/hadoop-3.3.0/<br>drwxr-xr-x 11 root  root        240 4月  29 10:31 hadoop-3.3.0<br>lrwxrwxrwx  1 root  root         27 3月  11 12:35 jdk -&gt; /export/server/jdk1.8.0_241<br>drwxr-xr-x  7 10143 10143       265 3月  11 12:42 jdk1.8.0_241<br>lrwxrwxrwx  1 root  root         31 3月  18 16:20 kafka -&gt; /export/server/kafka_2.11-2.0.0<br>drwxr-xr-x  7 root  root        101 3月  18 16:21 kafka_2.11-2.0.0<br>lrwxrwxrwx  1 root  root         40 3月  15 15:35 spark -&gt; /export/server/spark-3.2.0-bin-hadoop3.2<br>drwxr-xr-x 15  1000  1000       235 3月  22 16:08 spark-3.2.0-bin-hadoop3.2<br>lrwxrwxrwx  1 root  root         41 3月  29 16:49 zookeeper -&gt; /export/server/apache-zookeeper-3.7.0-bin<br>drwxr-xr-x 11  1000  1000      4096 3月  11 19:30 zookeeper-3.4.6</p></blockquote><h5 id="⑨添加node2、node3的myid配置：echo-2-gt-export-server-zookeeper-zkdatas-myid-echo-3-gt-export-server-zookeeper-zkdatas-myid"><a href="#⑨添加node2、node3的myid配置：echo-2-gt-export-server-zookeeper-zkdatas-myid-echo-3-gt-export-server-zookeeper-zkdatas-myid" class="headerlink" title="⑨添加node2、node3的myid配置：echo 2 &gt; /export/server/zookeeper/zkdatas/myid echo 3 &gt; /export/server/zookeeper/zkdatas/myid"></a>⑨添加node2、node3的myid配置：echo 2 &gt; /export/server/zookeeper/zkdatas/myid echo 3 &gt; /export/server/zookeeper/zkdatas/myid</h5><blockquote><p>(base) [root@node2 zkdatas]# cat myid<br>2</p></blockquote><blockquote><p>(base) [root@node3 zkdatas]# cat myid<br>3</p></blockquote><h5 id="⑩重新在-export-server-zookeeper-bin-目录下编写脚本一键启动"><a href="#⑩重新在-export-server-zookeeper-bin-目录下编写脚本一键启动" class="headerlink" title="⑩重新在/export/server/zookeeper/bin/目录下编写脚本一键启动"></a>⑩重新在/export/server/zookeeper/bin/目录下编写脚本一键启动</h5><h3 id="6、把node1上的master和worker关掉：-export-server-spark-sbin-stop-all-sh"><a href="#6、把node1上的master和worker关掉：-export-server-spark-sbin-stop-all-sh" class="headerlink" title="6、把node1上的master和worker关掉：/export/server/spark/sbin/stop-all.sh"></a>6、把node1上的master和worker关掉：/export/server/spark/sbin/stop-all.sh</h3><h3 id="7、把node2上的master关掉：-export-server-spark-sbin-stop-master-sh"><a href="#7、把node2上的master关掉：-export-server-spark-sbin-stop-master-sh" class="headerlink" title="7、把node2上的master关掉：/export/server/spark/sbin/stop-master.sh"></a>7、把node2上的master关掉：/export/server/spark/sbin/stop-master.sh</h3><h3 id="8、启动node1上的master和worker：-export-server-spark-sbin-start-all-sh"><a href="#8、启动node1上的master和worker：-export-server-spark-sbin-start-all-sh" class="headerlink" title="8、启动node1上的master和worker：/export/server/spark/sbin/start-all.sh"></a>8、启动node1上的master和worker：/export/server/spark/sbin/start-all.sh</h3><blockquote><p>(base) [root@node1 ~]# jps<br>66768 SparkSunmit<br>14578 Kafka<br>8389 NodeManager<br>11349 QuorumPeerMain<br>89049 Jps<br>22379 HistoryServer<br>3164 Master<br>61612 JobHistoryServer<br>6221 DataNode<br>5807 NameNode<br>7967 ResourceManager  </p></blockquote><h3 id="9、查看node1、node2的spark-web-UI页面"><a href="#9、查看node1、node2的spark-web-UI页面" class="headerlink" title="9、查看node1、node2的spark web UI页面"></a>9、查看node1、node2的spark web UI页面</h3><p><img src="../picture/HA%E5%90%8Enode1%E7%9A%848080.png"><br><img src="../picture/HA%E5%90%8Enode2%E7%9A%848080.png"></p><h3 id="10、启动node2上的master，将node1上的master-kill掉，查看node2的spark-web-UI页面"><a href="#10、启动node2上的master，将node1上的master-kill掉，查看node2的spark-web-UI页面" class="headerlink" title="10、启动node2上的master，将node1上的master kill掉，查看node2的spark web UI页面"></a>10、启动node2上的master，将node1上的master kill掉，查看node2的spark web UI页面</h3><p><img src="../picture/node1kill%E4%B9%8B%E5%90%8Enode2%E7%9A%848080.png"></p><h1 id="八、Spark（yarn）"><a href="#八、Spark（yarn）" class="headerlink" title="八、Spark（yarn）"></a>八、Spark（yarn）</h1><h3 id="1、启动yarn的历史服务器，jps看进程，查看web-UI"><a href="#1、启动yarn的历史服务器，jps看进程，查看web-UI" class="headerlink" title="1、启动yarn的历史服务器，jps看进程，查看web UI"></a>1、启动yarn的历史服务器，jps看进程，查看web UI</h3><blockquote><p>(base) [root@node1 ~]# jps<br>118256 JobHistoryServer<br>75877 Master<br>118471 Jps<br>69494 DataNode<br>70534 ResourceManager<br>70934 NodeManager<br>67992 NameNode<br>71402 HistoryServer<br><img src="../picture/yarn%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8.png">  </p></blockquote><h3 id="2、在yarn上启动pyspark"><a href="#2、在yarn上启动pyspark" class="headerlink" title="2、在yarn上启动pyspark"></a>2、在yarn上启动pyspark</h3><p><img src="../picture/yarn%E4%B8%8A%E5%90%AF%E5%8A%A8pyspark.png"></p><h3 id="3、命令测试"><a href="#3、命令测试" class="headerlink" title="3、命令测试"></a>3、命令测试</h3><blockquote><p>&gt;&gt;&gt; sc.parallelize([1,2,3,4,5]).map(lambda x: x * 10).collect()<br>[10, 20, 30, 40, 50]<br><img src="../picture/%E5%91%BD%E4%BB%A4%E6%B5%8B%E8%AF%95%E6%88%AA%E5%9B%BE1.png"><br><img src="../picture/%E5%91%BD%E4%BB%A4%E6%B5%8B%E8%AF%95%E6%88%AA%E5%9B%BE2.png"></p></blockquote><h3 id="4、提交任务测试"><a href="#4、提交任务测试" class="headerlink" title="4、提交任务测试"></a>4、提交任务测试</h3><blockquote><p>(base) [root@nodel ~]# cd /export/server/spark<br>(base) [root@nodel spark]# bin/spark-submit –master yarn /export/server/spark/examples/src/main/ python/pi.py 10<br>22/04/10 18:32:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>22/04/10 18:32:16 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling bac k to uploading libraries under SPARK HOME.<br>22/04/10 18:33:18 WARN YarnScheduler: Initial job has not accepted any resources; check your clus ter UI to ensure that workers are registered and have sufficient resources<br>Pi is roughly 3.141000<br><img src="../picture/%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1%E6%B5%8B%E8%AF%95%E6%88%AA%E5%9B%BE1.png"><br><img src="../picture/%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1%E6%B5%8B%E8%AF%95%E6%88%AA%E5%9B%BE2.png"></p></blockquote><h3 id="5、client模式测试pi"><a href="#5、client模式测试pi" class="headerlink" title="5、client模式测试pi"></a>5、client模式测试pi</h3><blockquote><p>(base) [root@nodel ~]#/export/server/spark/bin/spark-submit –master yarn –deploy-mode client一-driver-memory 512m –executor-memory 512m –num-executors 3 –total-executor-cores 3 /export/server/spark/examples/src/main/python/pi.py 3<br>22/04/10 18:35:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>22/04/10 18:35:27 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling bac k to uploading libraries under SPARK HOME.<br>22/04/10 18:36:45 WAPN YarnScheduler: Initial job has not accepted any resources; check your clus ter UI to ensure that workers are registered and have sufficient resources<br>Pi is roughly 3.140640<br><img src="../picture/client%E6%88%AA%E5%9B%BE1.png"><br><img src="../picture/client%E6%88%AA%E5%9B%BE2.png"></p></blockquote><h3 id="6、cluster模式测试pi"><a href="#6、cluster模式测试pi" class="headerlink" title="6、cluster模式测试pi"></a>6、cluster模式测试pi</h3><blockquote><p>(base) [root@nodel ~]#/export/server/spark/bin/spark-submit –master yarn –deploy-mode cluster –driver-memory 512m –executor-memory 512m –num-executors 3 –total-executor-cores 3 /export/server/spark/examples/src/main/python/pi.py 3<br>22/04/10 18:40:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>22/04/10 18:40:10 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling bac k to uploading libraries under SPARK HOME.<br>您在 /var/spool/mail/root 中有新邮件<br><img src="../picture/cluster%E6%88%AA%E5%9B%BE1.png"><br><img src="../picture/cluster%E6%88%AA%E5%9B%BE2.png"></p></blockquote><p>**</p></details><hr>]]></content>
    
    
    <summary type="html">Spark HA &amp; Yarn配置</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Spark local&amp;stand-alone配置</title>
    <link href="http://example.com/2022/05/27/Spark%20local&amp;%20stand-alone%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/2022/05/27/Spark%20local&amp;%20stand-alone%E9%85%8D%E7%BD%AE/</id>
    <published>2022-05-27T07:00:00.000Z</published>
    <updated>2022-06-19T10:34:20.824Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark-local-amp-stand-alone配置"><a href="#Spark-local-amp-stand-alone配置" class="headerlink" title="Spark local&amp;stand-alone配置"></a>Spark local&amp;stand-alone配置</h1><hr><p>title: Spark local&amp;stand-alone配置<br>date: 2022-05-27 15:00:00<br>description: Spark local&amp;stand-alone配置</p><details><summary>阅读全文</summary><h1 id="五、Spark（local）"><a href="#五、Spark（local）" class="headerlink" title="五、Spark（local）"></a>五、Spark（local）</h1><h3 id="1、将Anaconda3文件上传到-export-server-路径下，执行这个文件，将Anaconda3安装到-export-server-路径下，看到base开头证明安装成功"><a href="#1、将Anaconda3文件上传到-export-server-路径下，执行这个文件，将Anaconda3安装到-export-server-路径下，看到base开头证明安装成功" class="headerlink" title="1、将Anaconda3文件上传到/export/server/路径下，执行这个文件，将Anaconda3安装到/export/server/路径下，看到base开头证明安装成功"></a>1、将Anaconda3文件上传到/export/server/路径下，执行这个文件，将Anaconda3安装到/export/server/路径下，看到base开头证明安装成功</h3><blockquote><p>(base) [root@node1 server]#</p></blockquote><h3 id="2、配置-root-路径下-condarc文件"><a href="#2、配置-root-路径下-condarc文件" class="headerlink" title="2、配置/root/路径下.condarc文件"></a>2、配置/root/路径下.condarc文件</h3><blockquote><p>channels:  </p><ul><li>defaults<br>show_channel_urls: true<br>default_channels:  </li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</a>  </li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</a>  </li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</a><br>custom_channels:<br>conda-forge: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>msys2: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>bioconda: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>menpo: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>pytorch: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>simpleitk: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a></li></ul></blockquote><h3 id="3、创建虚拟环境pyspark，基于Python-3-8-：conda-create-n-pyspark-python-3-8"><a href="#3、创建虚拟环境pyspark，基于Python-3-8-：conda-create-n-pyspark-python-3-8" class="headerlink" title="3、创建虚拟环境pyspark，基于Python 3.8 ：conda create -n pyspark python=3.8"></a>3、创建虚拟环境pyspark，基于Python 3.8 ：conda create -n pyspark python=3.8</h3><h3 id="4、切换到虚拟环境内"><a href="#4、切换到虚拟环境内" class="headerlink" title="4、切换到虚拟环境内"></a>4、切换到虚拟环境内</h3><blockquote><p>(base) [root@node1 ~]# conda activate pyspark<br>您在 /var/spool/mail/root 中有新邮件<br>(pyspark) [root@node1 ~]#</p></blockquote><h3 id="5、在虚拟环境内安装包：pip-install-pyhive-pyspark-jieba-i-https-pypi-tuna-tsinghua-edu-cn-simple"><a href="#5、在虚拟环境内安装包：pip-install-pyhive-pyspark-jieba-i-https-pypi-tuna-tsinghua-edu-cn-simple" class="headerlink" title="5、在虚拟环境内安装包：pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple"></a>5、在虚拟环境内安装包：pip install pyhive pyspark jieba -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></h3><h3 id="6、将spark压缩包上传到-export-server-路径下，解压后删除压缩包，并设计软连接"><a href="#6、将spark压缩包上传到-export-server-路径下，解压后删除压缩包，并设计软连接" class="headerlink" title="6、将spark压缩包上传到/export/server/路径下，解压后删除压缩包，并设计软连接"></a>6、将spark压缩包上传到/export/server/路径下，解压后删除压缩包，并设计软连接</h3><blockquote><p>lrwxrwxrwx  1 root  root         40 3月  15 15:35 spark -&gt; /export/server/spark-3.2.0-bin-hadoop3.2<br>drwxr-xr-x 15  1000  1000       235 3月  22 16:08 spark-3.2.0-bin-hadoop3.2  </p></blockquote><h3 id="7、配置环境变量，加载环境变量"><a href="#7、配置环境变量，加载环境变量" class="headerlink" title="7、配置环境变量，加载环境变量"></a>7、配置环境变量，加载环境变量</h3><blockquote><p>#JAVA_HOME<br>export JAVA_HOME=/export/server/jdk1.8.0_241<br>export PATH=$PATH:$JAVA_HOME/bin<br>export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar<br>#HADOOP_HOME<br>export HADOOP_HOME=/export/server/hadoop-3.3.0<br>export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin  </p></blockquote><blockquote><p>#SPARK_HOME<br>export SPARK_HOME=/export/server/spark<br>#HADOOP_CONF_DIR<br>export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop<br>#PYSPARK_PYTHON<br>export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</p></blockquote><h3 id="8、配置-root-路径下-bashrc文件"><a href="#8、配置-root-路径下-bashrc文件" class="headerlink" title="8、配置/root/路径下.bashrc文件"></a>8、配置/root/路径下.bashrc文件</h3><blockquote><p>#JAVA_HOME<br>export JAVA_HOME=/export/server/jdk1.8.0_241<br>#PYSPARK_PYTHON<br>export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</p></blockquote><h3 id="9、进入-export-server-anaconda3-envs-pyspark-bin-路径下进行测试"><a href="#9、进入-export-server-anaconda3-envs-pyspark-bin-路径下进行测试" class="headerlink" title="9、进入/export/server/anaconda3/envs/pyspark/bin/路径下进行测试"></a>9、进入/export/server/anaconda3/envs/pyspark/bin/路径下进行测试</h3><p><img src="../picture/%E6%B5%8B%E8%AF%95.png"></p><h3 id="10、查看web-UI页面"><a href="#10、查看web-UI页面" class="headerlink" title="10、查看web UI页面"></a>10、查看web UI页面</h3><p><img src="../picture/webUI.png"></p><h1 id="六、Spark（Stand-alone）"><a href="#六、Spark（Stand-alone）" class="headerlink" title="六、Spark（Stand alone）"></a>六、Spark（Stand alone）</h1><h3 id="1、使用scp拷贝Anacanda文件、-condarc、-bashrc和环境变量到node2-node3上"><a href="#1、使用scp拷贝Anacanda文件、-condarc、-bashrc和环境变量到node2-node3上" class="headerlink" title="1、使用scp拷贝Anacanda文件、.condarc、.bashrc和环境变量到node2,node3上"></a>1、使用scp拷贝Anacanda文件、.condarc、.bashrc和环境变量到node2,node3上</h3><h4 id="（1）、创建虚拟环境pyspark，基于Python-3-8-：conda-create-n-pyspark-python-3-8"><a href="#（1）、创建虚拟环境pyspark，基于Python-3-8-：conda-create-n-pyspark-python-3-8" class="headerlink" title="（1）、创建虚拟环境pyspark，基于Python 3.8 ：conda create -n pyspark python=3.8"></a>（1）、创建虚拟环境pyspark，基于Python 3.8 ：conda create -n pyspark python=3.8</h4><h4 id="（2）、切换到虚拟环境内"><a href="#（2）、切换到虚拟环境内" class="headerlink" title="（2）、切换到虚拟环境内"></a>（2）、切换到虚拟环境内</h4><blockquote><p>(base) [root@node1 ~]# conda activate pyspark<br>您在 /var/spool/mail/root 中有新邮件<br>(pyspark) [root@node1 ~]#</p></blockquote><h4 id="（3）、在虚拟环境内安装包：pip-install-pyhive-pyspark-jieba-i-https-pypi-tuna-tsinghua-edu-cn-simple"><a href="#（3）、在虚拟环境内安装包：pip-install-pyhive-pyspark-jieba-i-https-pypi-tuna-tsinghua-edu-cn-simple" class="headerlink" title="（3）、在虚拟环境内安装包：pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple"></a>（3）、在虚拟环境内安装包：pip install pyhive pyspark jieba -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></h4><h3 id="2、将workers-template改名为workers，并配置内容"><a href="#2、将workers-template改名为workers，并配置内容" class="headerlink" title="2、将workers.template改名为workers，并配置内容"></a>2、将workers.template改名为workers，并配置内容</h3><blockquote><p><algorithm># A Spark Worker will be started on each of the machines listed below.<br>node1<br>node2<br>node3</p></blockquote><h3 id="3、将spark-env-sh-template改名为spark-env-sh，并配置内容"><a href="#3、将spark-env-sh-template改名为spark-env-sh，并配置内容" class="headerlink" title="3、将spark-env.sh.template改名为spark-env.sh，并配置内容"></a>3、将spark-env.sh.template改名为spark-env.sh，并配置内容</h3><blockquote><p><algorithm>## 设置JAVA安装目录<br>JAVA_HOME=/export/server/jdk  </p><p><algorithm>## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群<br>HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop<br>YARN_CONF_DIR=/export/server/hadoop/etc/hadoop  </p><p><algorithm>## 指定spark老大Master的IP和提交任务的通信端口<br><algorithm># 告知Spark的master运行在哪个机器上<br>#export SPARK_MASTER_HOST=node1<br><algorithm># 告知sparkmaster的通讯端口<br>export SPARK_MASTER_PORT=7077<br><algorithm># 告知spark master的 webui端口<br>SPARK_MASTER_WEBUI_PORT=8080  </p><p><algorithm># worker cpu可用核数<br>SPARK_WORKER_CORES=1<br><algorithm># worker可用内存<br>SPARK_WORKER_MEMORY=1g<br><algorithm># worker的工作通讯地址<br>SPARK_WORKER_PORT=7078<br><algorithm># worker的 webui地址<br>SPARK_WORKER_WEBUI_PORT=8081  </p><p><algorithm>## 设置历史服务器<br><algorithm># 配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中<br>SPARK_HISTORY_OPTS=”-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true”</p></blockquote><h3 id="4、Hadoop集群打开，将spark日志路径、权限上传到HDFS上：hadoop-fs-mkdir-sparklog"><a href="#4、Hadoop集群打开，将spark日志路径、权限上传到HDFS上：hadoop-fs-mkdir-sparklog" class="headerlink" title="4、Hadoop集群打开，将spark日志路径、权限上传到HDFS上：hadoop fs -mkdir /sparklog"></a>4、Hadoop集群打开，将spark日志路径、权限上传到HDFS上：hadoop fs -mkdir /sparklog</h3><blockquote><p>hadoop fs -chmod 777 /sparklog</p></blockquote><h3 id="5、将spark-defaults-conf-sh-template改名为spark-defaults-conf，并配置内容"><a href="#5、将spark-defaults-conf-sh-template改名为spark-defaults-conf，并配置内容" class="headerlink" title="5、将spark-defaults.conf.sh.template改名为spark-defaults.conf，并配置内容"></a>5、将spark-defaults.conf.sh.template改名为spark-defaults.conf，并配置内容</h3><blockquote><p><algorithm># 开启spark的日期记录功能<br>spark.eventLog.enabled         true<br><algorithm># 设置spark日志记录的路径<br>spark.eventLog.dir         hdfs://node1:8020/sparklog/<br><algorithm># 设置spark日志是否启动压缩<br>spark.eventLog.compress        true</p></blockquote><h3 id="6、将log4j-properties-template改名为log4j-properties，并配置内容"><a href="#6、将log4j-properties-template改名为log4j-properties，并配置内容" class="headerlink" title="6、将log4j.properties.template改名为log4j.properties，并配置内容"></a>6、将log4j.properties.template改名为log4j.properties，并配置内容</h3><blockquote><p>log4j.rootCategory=WARN, console<br>log4j.appender.console=org.apache.log4j.ConsoleAppender<br>log4j.appender.console.target=System.err<br>log4j.appender.console.layout=org.apache.log4j.PatternLayout<br>log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n</p></blockquote><h3 id="7、使用scp拷贝spark到node2-node3上"><a href="#7、使用scp拷贝spark到node2-node3上" class="headerlink" title="7、使用scp拷贝spark到node2,node3上"></a>7、使用scp拷贝spark到node2,node3上</h3><h3 id="8、在node2、node3上给spark安装目录增加软连接"><a href="#8、在node2、node3上给spark安装目录增加软连接" class="headerlink" title="8、在node2、node3上给spark安装目录增加软连接"></a>8、在node2、node3上给spark安装目录增加软连接</h3><h3 id="9、启动历史服务器-export-server-spark-sbin-start-history-server-sh"><a href="#9、启动历史服务器-export-server-spark-sbin-start-history-server-sh" class="headerlink" title="9、启动历史服务器/export/server/spark/sbin/start-history-server.sh"></a>9、启动历史服务器/export/server/spark/sbin/start-history-server.sh</h3><blockquote><p>(base) [root@node1 server]# jps<br>69494 DataNode<br>70534 ResourceManager<br>70934 NodeManager<br>67992 NameNode<br>72075 Jps<br>71402 HistoryServer</p></blockquote><h3 id="10、启动spark的master和worker进程：sbin-start-all-sh"><a href="#10、启动spark的master和worker进程：sbin-start-all-sh" class="headerlink" title="10、启动spark的master和worker进程：sbin/start-all.sh"></a>10、启动spark的master和worker进程：sbin/start-all.sh</h3><blockquote><p>(base) [root@node1 server]# jps<br>75877 Master<br>76372 Jps<br>69494 DataNode<br>70534 ResourceManager<br>70934 NodeManager<br>67992 NameNode<br>71402 HistoryServer<br>76093 Worker</p></blockquote><h3 id="11、查看spark-web-UI页面（8080）、历史服务器web-UI页面（18080）"><a href="#11、查看spark-web-UI页面（8080）、历史服务器web-UI页面（18080）" class="headerlink" title="11、查看spark web UI页面（8080）、历史服务器web UI页面（18080）"></a>11、查看spark web UI页面（8080）、历史服务器web UI页面（18080）</h3><p><img src="../picture/8080.png"><br><img src="../picture/18080.png"></p><p>**</p></details><hr>]]></content>
    
    
    <summary type="html">Spark local&amp;stand-alone配置</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Spark基础环境配置</title>
    <link href="http://example.com/2022/05/27/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/2022/05/27/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</id>
    <published>2022-05-27T00:00:00.000Z</published>
    <updated>2022-06-19T10:34:16.820Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark基础环境配置"><a href="#Spark基础环境配置" class="headerlink" title="Spark基础环境配置"></a>Spark基础环境配置</h1><hr><p>title: Spark基础环境配置<br>date: 2022-05-27 08:00:00<br>description: Spark基础环境配置</p><details><summary>阅读全文</summary> <h1 id="一、基础环境"><a href="#一、基础环境" class="headerlink" title="一、基础环境"></a>一、基础环境</h1><h3 id="1、导入三台虚拟机，三台虚拟机信息汇总"><a href="#1、导入三台虚拟机，三台虚拟机信息汇总" class="headerlink" title="1、导入三台虚拟机，三台虚拟机信息汇总"></a>1、导入三台虚拟机，三台虚拟机信息汇总</h3><table><thead><tr><th align="center">主机名</th><th align="center">node1.itcast.cn</th><th align="center">node2.itcast.cn</th><th align="center">node3.iecast.cn</th></tr></thead><tbody><tr><td align="center">IP</td><td align="center">192.168.88.151</td><td align="center">192.168.88.152</td><td align="center">192.168.88.153</td></tr><tr><td align="center">用户名、密码</td><td align="center">root/123456</td><td align="center">root/123456</td><td align="center">root/123456</td></tr></tbody></table><h3 id="2、集群角色规划"><a href="#2、集群角色规划" class="headerlink" title="2、集群角色规划"></a>2、集群角色规划</h3><table><thead><tr><th align="center">服务器</th><th align="center">运行角色</th></tr></thead><tbody><tr><td align="center">node1.itcast.cn</td><td align="center">namenode（主角色） datanode（从角色） resourcemanager（主角色） nodemanager（从角色）</td></tr><tr><td align="center">node2.itcast.cn</td><td align="center">secondarynamenode（主角色辅助角色） datanode（从角色） nodemanager（从角色）</td></tr><tr><td align="center">node3.itcast.cn</td><td align="center">datanode（从角色） nodemanager（从角色）</td></tr></tbody></table><h3 id="3、hosts映射"><a href="#3、hosts映射" class="headerlink" title="3、hosts映射"></a>3、hosts映射</h3><blockquote><p>[root@node1 etc]# cat hosts<br>127.0.0.1  localhost localhost.localdomain localhost4.localdomain4<br>::1        localhost localhost.localdomain localhost6.localdomain6<br>192.168.88.151 node1 node1.itcast.cn<br>192.168.88.152 node2 node2.itcast.cn<br>192.168.88.153 node3 node3.itcast.cn</p></blockquote><h3 id="4、快捷命令"><a href="#4、快捷命令" class="headerlink" title="4、快捷命令"></a>4、快捷命令</h3><h4 id="1-、查看主机名：cat-etc-hostname"><a href="#1-、查看主机名：cat-etc-hostname" class="headerlink" title="(1)、查看主机名：cat /etc/hostname"></a>(1)、查看主机名：cat /etc/hostname</h4><blockquote><p>[root@node1 ~]# cat /etc/hostname<br>node1.itcast.cn</p></blockquote><h4 id="（2）、查看hosts映射：cat-etc-hosts"><a href="#（2）、查看hosts映射：cat-etc-hosts" class="headerlink" title="（2）、查看hosts映射：cat /etc/hosts"></a>（2）、查看hosts映射：cat /etc/hosts</h4><blockquote><p>[root@node1 etc]# cat hosts<br>127.0.0.1  localhost localhost.localdomain localhost4.localdomain4<br>::1        localhost localhost.localdomain localhost6.localdomain6<br>192.168.88.151 node1 node1.itcast.cn<br>192.168.88.152 node2 node2.itcast.cn<br>192.168.88.153 node3 node3.itcast.cn</p></blockquote><h4 id="（3）、查看防火墙状态：systemctl-status-firewalld-service"><a href="#（3）、查看防火墙状态：systemctl-status-firewalld-service" class="headerlink" title="（3）、查看防火墙状态：systemctl status firewalld.service"></a>（3）、查看防火墙状态：systemctl status firewalld.service</h4><blockquote><p>[root@node1 ~]# systemctl status firewalld.service<br>firewalld.service - firewalld - dynamic firewall daemon<br>Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)<br>Active：inactive (dead)<br>Docs: man:firewalld(1)  </p></blockquote><h4 id="（4）、登录node1：ssh-node1-登录node2：ssh-node2-登录node3：ssh-node3"><a href="#（4）、登录node1：ssh-node1-登录node2：ssh-node2-登录node3：ssh-node3" class="headerlink" title="（4）、登录node1：ssh node1    登录node2：ssh node2   登录node3：ssh node3"></a>（4）、登录node1：ssh node1    登录node2：ssh node2   登录node3：ssh node3</h4><blockquote><p>[root@node1<del>]# ssh node1<br>Last login:Sat Apr 9 15:54:40 2022 from 192.168.88.1<br>[root@node1</del>]# ssh node2<br>Last login:Sat Apr 9 15:54:43 2022 from 192.168.88.1<br>[root@node2~]# ssh node3<br>Last login:Sat Apr 9 15:54:45 2022 from 192.168.88.1  </p></blockquote><h4 id="5-、同步时间：ntpdate-ntp5-aliyun-com"><a href="#5-、同步时间：ntpdate-ntp5-aliyun-com" class="headerlink" title="(5)、同步时间：ntpdate ntp5.aliyun.com"></a>(5)、同步时间：ntpdate ntp5.aliyun.com</h4><blockquote><p>[root@node1~]# ntpdate ntp5.aliyun.com<br>9 Apr 16:10:36 ntpdate[37514]: adjust time server 203.107.6.88 offset -0.000747 sec</p></blockquote><h1 id="二、JDK"><a href="#二、JDK" class="headerlink" title="二、JDK"></a>二、JDK</h1><h3 id="1、将jdk压缩包上传到-export-server-路径下，解压后删除压缩包，并设计软连接"><a href="#1、将jdk压缩包上传到-export-server-路径下，解压后删除压缩包，并设计软连接" class="headerlink" title="1、将jdk压缩包上传到/export/server/路径下，解压后删除压缩包，并设计软连接"></a>1、将jdk压缩包上传到/export/server/路径下，解压后删除压缩包，并设计软连接</h3><blockquote><p>lrwxrwxrwx  1 root  root         27 3月  11 12:35 jdk -&gt; /export/server/jdk1.8.0_241<br>drwxr-xr-x  7 10143 10143       265 3月  11 12:42 jdk1.8.0_241  </p></blockquote><h3 id="2、配置环境变量，重新加载环境变量"><a href="#2、配置环境变量，重新加载环境变量" class="headerlink" title="2、配置环境变量，重新加载环境变量"></a>2、配置环境变量，重新加载环境变量</h3><blockquote><p>#JAVA_HOME<br>export JAVA_HOME=/export/server/jdk1.8.0_241<br>export PATH=$PATH:$JAVA_HOME/bin<br>export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar  </p></blockquote><h3 id="3、使用scp拷贝jdk到node2-node3上，配置node2，node3的环境变量，重新加载环境变量"><a href="#3、使用scp拷贝jdk到node2-node3上，配置node2，node3的环境变量，重新加载环境变量" class="headerlink" title="3、使用scp拷贝jdk到node2,node3上，配置node2，node3的环境变量，重新加载环境变量"></a>3、使用scp拷贝jdk到node2,node3上，配置node2，node3的环境变量，重新加载环境变量</h3><h3 id="4、查看java状态"><a href="#4、查看java状态" class="headerlink" title="4、查看java状态"></a>4、查看java状态</h3><blockquote><p>[root@node1 ~]# java -version<br>java version “1.8.0_241”<br>Java(TM) SE Runtime Environment (build 1.8.0_241-b07)<br>Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)</p></blockquote><h1 id="三、Hadoop"><a href="#三、Hadoop" class="headerlink" title="三、Hadoop"></a>三、Hadoop</h1><h3 id="1、将Hadoop压缩包上传到-export-server-路径下，解压后删除压缩包，并设计软连接"><a href="#1、将Hadoop压缩包上传到-export-server-路径下，解压后删除压缩包，并设计软连接" class="headerlink" title="1、将Hadoop压缩包上传到/export/server/路径下，解压后删除压缩包，并设计软连接"></a>1、将Hadoop压缩包上传到/export/server/路径下，解压后删除压缩包，并设计软连接</h3><blockquote><p>lrwxrwxrwx  1 root  root         28 3月  11 14:51 hadoop -&gt; /export/server/hadoop-3.3.0/<br>drwxr-xr-x 11 root  root        240 4月  29 10:31 hadoop-3.3.0 </p></blockquote><h3 id="2、编辑Hadoop配置文件"><a href="#2、编辑Hadoop配置文件" class="headerlink" title="2、编辑Hadoop配置文件"></a>2、编辑Hadoop配置文件</h3><h4 id="（1）、配置hadoop-env-sh"><a href="#（1）、配置hadoop-env-sh" class="headerlink" title="（1）、配置hadoop-env.sh"></a>（1）、配置hadoop-env.sh</h4><blockquote><p>export JAVA_HOME=/export/server/jdk1.8.0_241  </p><p>export HDFS_NAMENODE_USER=root<br>export HDFS_DATANODE_USER=root<br>export HDFS_SECONDARYNAMENODE_USER=root<br>export YARN_RESOURCEMANAGER_USER=root<br>export YARN_NODEMANAGER_USER=root  </p></blockquote><h4 id="（2）、配置core-site-xml"><a href="#（2）、配置core-site-xml" class="headerlink" title="（2）、配置core-site.xml"></a>（2）、配置core-site.xml</h4><blockquote><p>&lt;!– 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt;fs.defaultFS&lt;/name&gt;<br>      &lt;value&gt;hdfs://node1:8020&lt;/value&gt;<br>&lt;/property&gt;  </p><p>&lt;!– 设置Hadoop本地保存数据路径 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br>      &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;<br>&lt;/property&gt;  </p><p>&lt;!– 设置HDFS web UI用户身份 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;<br>      &lt;value&gt;root&lt;/value&gt;<br>&lt;/property&gt;  </p><p>&lt;!– 整合hive 用户代理设置 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt; hadoop.proxyuser.root.hosts&lt;/name&gt;<br>      &lt;value&gt; *&lt;/value&gt;<br>&lt;/property&gt;   </p><p>&lt;property&gt;<br>      &lt;name&gt; hadoop.proxyuser.root.groups&lt;/name&gt;<br>      &lt;value&gt; *&lt;/value&gt;<br>&lt;/property&gt;   </p><p>&lt;!– 文件系统垃圾桶保存时间 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt; fs.trash.interval&lt;/name&gt;<br>      &lt;value&gt; 1440&lt;/value&gt;<br>&lt;/property&gt;  </p></blockquote><h4 id="（3）、配置hdfs-site-xml"><a href="#（3）、配置hdfs-site-xml" class="headerlink" title="（3）、配置hdfs-site.xml"></a>（3）、配置hdfs-site.xml</h4><blockquote><p>&lt;!– 设置SNN进程运行机器位置信息 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;<br>      &lt;value&gt;node2:9868&lt;/value&gt;<br>&lt;/property&gt;</p></blockquote><h4 id="（4）、配置mapred-site-xml"><a href="#（4）、配置mapred-site-xml" class="headerlink" title="（4）、配置mapred-site.xml"></a>（4）、配置mapred-site.xml</h4><blockquote><p>&lt;!– 设置MR程序默认运行模式： yarn集群模式 local本地模式 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br>      &lt;value&gt;yarn&lt;/value&gt;<br>&lt;/property&gt;  </p><p>&lt;!– MR程序历史服务地址 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;<br>      &lt;value&gt;node1:10020&lt;/value&gt;<br>&lt;/property&gt;  </p><p>&lt;!– MR程序历史服务器web端地址 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;<br>      &lt;value&gt;node1:19888&lt;/value&gt;<br>&lt;/property&gt;  </p><p>&lt;property&gt;<br>      &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;<br>      &lt;value&gt;HADOOP_MAPRED_HOME=${HADOOP_HOME}&lt;/value&gt;<br>&lt;/property&gt;  </p><p>&lt;property&gt;<br>      &lt;name&gt;mapreduce.map.env&lt;/name&gt;<br>      &lt;value&gt;HADOOP_MAPRED_HOME=${HADOOP_HOME}&lt;/value&gt;<br>&lt;/property&gt;  </p><p>&lt;property&gt;<br>      &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;<br>      &lt;value&gt;HADOOP_MAPRED_HOME=${HADOOP_HOME}&lt;/value&gt;<br>&lt;/property&gt;</p></blockquote><h4 id="（5）、配置yarn-site-xml"><a href="#（5）、配置yarn-site-xml" class="headerlink" title="（5）、配置yarn-site.xml"></a>（5）、配置yarn-site.xml</h4><blockquote><p>&lt;!– 设置YARN集群主角色运行机器位置 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br>      &lt;value&gt;node1&lt;/value&gt;<br>&lt;/property&gt;  </p><p>&lt;property&gt;<br>      &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br>      &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br>&lt;/property&gt;  </p><p>&lt;!– 是否将对容器实施物理内存限制 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;<br>      &lt;value&gt;false&lt;/value&gt;<br>&lt;/property&gt;  </p><p>&lt;!– 是否将对容器实施虚拟内存限制。 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;<br>      &lt;value&gt;false&lt;/value&gt;<br>&lt;/property&gt;  </p><p>&lt;!– 开启日志聚集 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;<br>      &lt;value&gt;true&lt;/value&gt;<br>&lt;/property&gt;  </p><p>&lt;!– 设置yarn历史服务器地址 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt;yarn.log.server.url&lt;/name&gt;<br>      &lt;value&gt;<a href="http://node1:19888/jobhistory/logs&lt;/value">http://node1:19888/jobhistory/logs&lt;/value</a>&gt;<br>&lt;/property&gt;  </p><p>&lt;!– 历史日志保存的时间 7天 –&gt;<br>&lt;property&gt;<br>      &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;<br>      &lt;value&gt;604800&lt;/value&gt;<br>&lt;/property&gt;</p></blockquote><h4 id="（6）、配置workers"><a href="#（6）、配置workers" class="headerlink" title="（6）、配置workers"></a>（6）、配置workers</h4><blockquote><p>node1.itcast.cn<br>node2.itcast.cn<br>node3.itcast.cn</p></blockquote><h3 id="3、使用scp拷贝Hadoop安装包到node2-node3上"><a href="#3、使用scp拷贝Hadoop安装包到node2-node3上" class="headerlink" title="3、使用scp拷贝Hadoop安装包到node2,node3上"></a>3、使用scp拷贝Hadoop安装包到node2,node3上</h3><h3 id="4、配置环境变量，重新加载环境变量"><a href="#4、配置环境变量，重新加载环境变量" class="headerlink" title="4、配置环境变量，重新加载环境变量"></a>4、配置环境变量，重新加载环境变量</h3><blockquote><p>#HADOOP_HOME<br>export HADOOP_HOME=/export/server/hadoop-3.3.0<br>export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</p></blockquote><h3 id="5、使用scp拷贝环境变量到node2，node3上"><a href="#5、使用scp拷贝环境变量到node2，node3上" class="headerlink" title="5、使用scp拷贝环境变量到node2，node3上"></a>5、使用scp拷贝环境变量到node2，node3上</h3><h3 id="6、首次启动格式化namenode：hdfs-namenode-format"><a href="#6、首次启动格式化namenode：hdfs-namenode-format" class="headerlink" title="6、首次启动格式化namenode：hdfs namenode -format"></a>6、首次启动格式化namenode：hdfs namenode -format</h3><h3 id="7、启动Hadoop，jps查看进程"><a href="#7、启动Hadoop，jps查看进程" class="headerlink" title="7、启动Hadoop，jps查看进程"></a>7、启动Hadoop，jps查看进程</h3><blockquote><p>[root@nodel server]#start-all.sh<br>Starting namenodes on [node1]<br>上一次登录：六 4月 9 16:08:52 CST 2022从 192.168.88.151pts/2 上<br>Starting datanodes<br>上一次登录：六 4月 9 17:30:31 CST 2022pts/2 上<br>Starting secondary namenodes [node2]<br>上一次登录：六 4月 9 17:30:34 CST 2022pts/2 上<br>Starting resourcemanager<br>上一次登录：六 4月 9 17:30:47 CST 2022pts/2 上<br>Starting nodemanagers<br>上一次登录：六 4月 9 17:31:01 CST 2022pts/2 上</p></blockquote><blockquote><p>[root@node1 server]# jps<br>88113 DataNode<br>87682 NameNode<br>89764 NodeManager<br>89319 ResourceManager<br>91035 Jps</p></blockquote><blockquote><p>[root@node2 server]# jps<br>86567 Jps<br>84298 SecondaryNameNode<br>84778 NodeManager<br>83724 DataNode</p></blockquote><blockquote><p>[root@node3 server]# jps<br>84524 NodeManager<br>86863 Jps<br>83566 DataNode</p></blockquote><h3 id="8、查看hdfs、yarn集群的web-UI页面"><a href="#8、查看hdfs、yarn集群的web-UI页面" class="headerlink" title="8、查看hdfs、yarn集群的web UI页面"></a>8、查看hdfs、yarn集群的web UI页面</h3><p><img src="../picture/hdfswebUI.png"><br><img src="../picture/yarnwebUI.png"></p><h1 id="四、Zookeeper"><a href="#四、Zookeeper" class="headerlink" title="四、Zookeeper"></a>四、Zookeeper</h1><h3 id="1、基本规划"><a href="#1、基本规划" class="headerlink" title="1、基本规划"></a>1、基本规划</h3><table><thead><tr><th align="center">服务器IP</th><th align="center">主机名</th><th align="center">myid的值</th></tr></thead><tbody><tr><td align="center">192.168.88.151</td><td align="center">node1</td><td align="center">1</td></tr><tr><td align="center">192.168.88.152</td><td align="center">node2</td><td align="center">2</td></tr><tr><td align="center">192.168.88.153</td><td align="center">node3</td><td align="center">3</td></tr></tbody></table><h3 id="2、将zookeeper压缩包上传到-export-server-路径下，解压后删除压缩包，并设计软连接"><a href="#2、将zookeeper压缩包上传到-export-server-路径下，解压后删除压缩包，并设计软连接" class="headerlink" title="2、将zookeeper压缩包上传到/export/server/路径下，解压后删除压缩包，并设计软连接"></a>2、将zookeeper压缩包上传到/export/server/路径下，解压后删除压缩包，并设计软连接</h3><blockquote><p>lrwxrwxrwx  1 root  root         41 3月  29 16:49 zookeeper -&gt; /export/server/apache-zookeeper-3.7.0-bin<br>drwxr-xr-x 11  1000  1000      4096 3月  11 19:30 zookeeper-3.4.6</p></blockquote><h3 id="3、编辑zookeeper配置文件"><a href="#3、编辑zookeeper配置文件" class="headerlink" title="3、编辑zookeeper配置文件"></a>3、编辑zookeeper配置文件</h3><h4 id="（1）、在-export-server-zookeeper-conf-目录下复制zoo-sample-cfg文件为zoo-cfg：cp-zoo-sample-cfg-zoo-cfg"><a href="#（1）、在-export-server-zookeeper-conf-目录下复制zoo-sample-cfg文件为zoo-cfg：cp-zoo-sample-cfg-zoo-cfg" class="headerlink" title="（1）、在/export/server/zookeeper/conf/目录下复制zoo_sample.cfg文件为zoo.cfg：cp zoo_sample.cfg zoo.cfg"></a>（1）、在/export/server/zookeeper/conf/目录下复制zoo_sample.cfg文件为zoo.cfg：cp zoo_sample.cfg zoo.cfg</h4><blockquote><p>[root@node1 conf]# ll<br>总用量 16<br>-rw-r–r– 1 1000 1000  535 3月  17 2021 configuration.xsl<br>-rw-r–r– 1 1000 1000 3435 3月  17 2021 log4j.properties<br>-rw-r–r– 1 root root 1266 4月  10 16:59 zoo.cfg<br>-rw-r–r– 1 1000 1000 1148 3月  17 2021 zoo_sample.cfg</p></blockquote><h4 id="（2）、在-export-server-zookeeper-路径下创建一个zkdatas文件夹：mkdir-p-export-server-zookeeper-zkdatas"><a href="#（2）、在-export-server-zookeeper-路径下创建一个zkdatas文件夹：mkdir-p-export-server-zookeeper-zkdatas" class="headerlink" title="（2）、在/export/server/zookeeper/路径下创建一个zkdatas文件夹：mkdir -p /export/server/zookeeper/zkdatas/"></a>（2）、在/export/server/zookeeper/路径下创建一个zkdatas文件夹：mkdir -p /export/server/zookeeper/zkdatas/</h4><h4 id="（3）、配置zoo-cfg"><a href="#（3）、配置zoo-cfg" class="headerlink" title="（3）、配置zoo.cfg"></a>（3）、配置zoo.cfg</h4><blockquote><p><algorithm># The number of milliseconds of each tick<br>tickTime=2000<br><algorithm># The number of ticks that the initial<br><algorithm># synchronization phase can take<br>initLimit=10<br><algorithm># The number of ticks that can pass between<br><algorithm># sending a request and getting an acknowledgement<br>syncLimit=5<br><algorithm># the directory where the snapshot is stored.<br><algorithm># do not use /tmp for storage, /tmp here is just<br><algorithm># example sakes.<br>dataDir=/export/server/zookeeper/zkdatas</p></blockquote><blockquote><p><algorithm># The number of snapshots to retain in dataDir<br>#autopurge.snapRetainCount=3<br><algorithm># Purge task interval in hours<br><algorithm># Set to “0” to disable auto purge feature<br>#autopurge.purgeInterval=1  </p><p><algorithm>## Metrics Providers<br><algorithm>#<br><algorithm># <a href="https://prometheus.io/">https://prometheus.io</a> Metrics Exporter<br>#metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider<br>#metricsProvider.httpPort=7000<br>#metricsProvider.exportJvmInfo=true<br>#集群中服务器地址<br>server.1=node1:2888:3888<br>server.2=node2:2888:3888<br>server.3=node3:2888:3888</p></blockquote><h4 id="（4）、在node1的-export-server-zookeeper-zkdatas-目录下创建一个myid文件，文件内容为1：echo-1-gt-export-server-zookeeper-zkdatas-myid"><a href="#（4）、在node1的-export-server-zookeeper-zkdatas-目录下创建一个myid文件，文件内容为1：echo-1-gt-export-server-zookeeper-zkdatas-myid" class="headerlink" title="（4）、在node1的/export/server/zookeeper/zkdatas/目录下创建一个myid文件，文件内容为1：echo 1 &gt; /export/server/zookeeper/zkdatas/myid"></a>（4）、在node1的/export/server/zookeeper/zkdatas/目录下创建一个myid文件，文件内容为1：echo 1 &gt; /export/server/zookeeper/zkdatas/myid</h4><blockquote><p>[root@node1 zkdatas]# cat myid<br>1</p></blockquote><h3 id="4、使用scp拷贝zookeeper安装包到node2-node3上"><a href="#4、使用scp拷贝zookeeper安装包到node2-node3上" class="headerlink" title="4、使用scp拷贝zookeeper安装包到node2,node3上"></a>4、使用scp拷贝zookeeper安装包到node2,node3上</h3><h3 id="5、在node2、node3上建立软连接"><a href="#5、在node2、node3上建立软连接" class="headerlink" title="5、在node2、node3上建立软连接"></a>5、在node2、node3上建立软连接</h3><h3 id="6、修改node2、node3的myid值为2、3"><a href="#6、修改node2、node3的myid值为2、3" class="headerlink" title="6、修改node2、node3的myid值为2、3"></a>6、修改node2、node3的myid值为2、3</h3><blockquote><p>[root@node2 zkdatas]# cat myid<br>2</p></blockquote><blockquote><p>[root@node3 zkdatas]# cat myid<br>3</p></blockquote><h3 id="7、配置环境变量，重新加载环境变量"><a href="#7、配置环境变量，重新加载环境变量" class="headerlink" title="7、配置环境变量，重新加载环境变量"></a>7、配置环境变量，重新加载环境变量</h3><blockquote><p>#ZOOKEEPER_HOME<br>export ZOOKEEPER_HOME=/export/server/zookeeper<br>export PATH=$PATH:$ZOOKEEPER_HOME/bin</p></blockquote><h3 id="8、三台机器分别启动zookeeper、查看zookeeper状态"><a href="#8、三台机器分别启动zookeeper、查看zookeeper状态" class="headerlink" title="8、三台机器分别启动zookeeper、查看zookeeper状态"></a>8、三台机器分别启动zookeeper、查看zookeeper状态</h3><h3 id="9、在-export-server-zookeeper-bin-目录下编写脚本一键启动"><a href="#9、在-export-server-zookeeper-bin-目录下编写脚本一键启动" class="headerlink" title="9、在/export/server/zookeeper/bin/目录下编写脚本一键启动"></a>9、在/export/server/zookeeper/bin/目录下编写脚本一键启动</h3><blockquote><p>[root@node1 ~]# /export/server/zookeeper/bin/zkall.sh start<br>starting node1<br>ZooKeeper JMX enabled by default<br>Using config: /export/server/zookeeper/bin/../conf/zoo.cfg<br>Starting zookeeper … STARTED<br>starting node2<br>ZooKeeper JMX enabled by default<br>Using config: /export/server/zookeeper/bin/../conf/zoo.cfg<br>Starting zookeeper … STARTED<br>starting node3<br>ZooKeeper JMX enabled by default<br>Using config: /export/server/zookeeper/bin/../conf/zoo.cfg<br>Starting zookeeper … STARTED</p></blockquote><h3 id="10、jps查看进程"><a href="#10、jps查看进程" class="headerlink" title="10、jps查看进程"></a>10、jps查看进程</h3><blockquote><p>[root@node1 ~]# jps<br>88113 DataNode<br>13457 QuorumPeerMain<br>87682 NameNode<br>89764 NodeManager<br>89319 ResourceManager<br>26486 Jps</p></blockquote><blockquote><p>[root@node2 ~]# jps<br>19489 Jps<br>5992 QuorumPeerMain<br>84298 SecondaryNameNode<br>84778 NodeManager<br>83724 DataNode</p></blockquote><blockquote><p>[root@node3 ~]# jps<br>5412 QuorumPeerMain<br>19516 Jps<br>84524 NodeManager<br>83566 DataNode</p></blockquote><p>**</p></details><hr>]]></content>
    
    
    <summary type="html">Spark基础环境配置</summary>
    
    
    
    
  </entry>
  
</feed>
